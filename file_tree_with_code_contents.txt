├── .gitignore
├── config.yaml
    --- Start of config.yaml ---
    api_enabled: true
    api_host: 127.0.0.1
    api_port: 5000
    auto_cycle: false
    components:
      crawl:
        delay: 1.5
        max_pages: 50
        seed_urls:
        - https://en.wikipedia.org/wiki/Artificial_intelligence
        - https://en.wikipedia.org/wiki/Biology
        - https://en.wikipedia.org/wiki/Biotechnology
        - https://en.wikipedia.org/wiki/Cluster_analysis
        - https://en.wikipedia.org/wiki/Machine_learning
        - https://en.wikipedia.org/wiki/Dimensionality_reduction
      harvester: {}
      meatsnake: {}
      mimic:
        num_samples: 3
      trimmings: {}
    cycle_interval: 86400
    data_dir: carnis_data
    log_level: INFO
    --- End of config.yaml ---
├── crawl.py
    --- Start of crawl.py ---
    import requests
    from bs4 import BeautifulSoup
    import json
    import time
    import random
    import urllib.robotparser
    from urllib.parse import urlparse, urljoin
    
    class Crawler:
        def __init__(self, seed_urls, max_pages=100, delay=1.0):
            """
            Initialize the crawler with seed URLs and constraints.
            
            Args:
                seed_urls (list): Initial URLs to start crawling from
                max_pages (int): Maximum number of pages to crawl
                delay (float): Delay between requests in seconds
            """
            self.seed_urls = seed_urls
            self.max_pages = max_pages
            self.delay = delay
            self.visited = set()
            self.to_visit = set(seed_urls)
            self.crawled_data = []
            self.robot_parsers = {}
        
        def _is_allowed(self, url):
            """Check if URL is allowed by robots.txt"""
            parsed_url = urlparse(url)
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            if base_url not in self.robot_parsers:
                rp = urllib.robotparser.RobotFileParser()
                rp.set_url(urljoin(base_url, "/robots.txt"))
                try:
                    rp.read()
                    self.robot_parsers[base_url] = rp
                except Exception:
                    # If robots.txt can't be read, assume crawling is allowed
                    return True
            
            return self.robot_parsers[base_url].can_fetch("*", url)
        
        def _extract_links(self, soup, base_url):
            """Extract links from a page"""
            links = set()
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                full_url = urljoin(base_url, href)
                # Filter for only http/https URLs
                if full_url.startswith(('http://', 'https://')):
                    links.add(full_url)
            return links
        
        def crawl(self):
            """Start the crawling process"""
            crawl_count = 0
            
            while self.to_visit and crawl_count < self.max_pages:
                # Get a URL from the queue
                url = self.to_visit.pop()
                if url in self.visited:
                    continue
                    
                if not self._is_allowed(url):
                    print(f"Skipping {url} (disallowed by robots.txt)")
                    continue
                    
                try:
                    # Introduce delay
                    time.sleep(self.delay + random.uniform(0.1, 0.5))
                    
                    # Make the request
                    print(f"Crawling: {url}")
                    response = requests.get(url, headers={
                        'User-Agent': 'VitaCarnisResearchBot/1.0'
                    }, timeout=10)
                    
                    if response.status_code == 200:
                        # Parse the page
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # Store data
                        title = soup.title.string if soup.title else "No title"
                        
                        # Extract main content (simplistic approach)
                        # Remove script and style elements
                        for script in soup(["script", "style"]):
                            script.extract()
                            
                        # Get text
                        content = soup.get_text(separator=' ', strip=True)
                        
                        # Store the crawled data
                        self.crawled_data.append({
                            'url': url,
                            'title': title,
                            'content': content,
                            'timestamp': time.time()
                        })
                        
                        # Find links on the page
                        new_links = self._extract_links(soup, url)
                        
                        # Add new links to the queue
                        for link in new_links:
                            if link not in self.visited:
                                self.to_visit.add(link)
                        
                        # Mark URL as visited
                        self.visited.add(url)
                        crawl_count += 1
                        print(f"Processed {url} ({crawl_count}/{self.max_pages})")
                        
                except Exception as e:
                    print(f"Error crawling {url}: {e}")
                    
            # Save results
            self.save_results()
            return self.crawled_data
        
        def save_results(self):
            """Save the crawled data to a JSON file in the carnis_data folder"""
            import os
            
            # Ensure the carnis_data directory exists
            os.makedirs('carnis_data', exist_ok=True)
    
            # Ensure the crawl directory with the crawled_data file exists
            os.makedirs(os.path.join('carnis_data', 'crawl'), exist_ok=True)
            
            # Save the data to the carnis_data folder
            output_path = os.path.join('carnis_data', 'crawl', 'crawled_data.json')
            with open(output_path, 'w') as f:
                json.dump(self.crawled_data, f, indent=4)
            print(f"Crawl complete. Collected data from {len(self.crawled_data)} pages.")
            print(f"Data saved to {output_path}")
    
    if __name__ == "__main__":
        # Example usage
        seeds = [
            "https://en.wikipedia.org/wiki/Artificial_intelligence",
            "https://en.wikipedia.org/wiki/Biology",
            "https://en.wikipedia.org/wiki/Biotechnology"
        ]
        
        crawler = Crawler(seeds, max_pages=50, delay=1.5)
        crawler.crawl()
    --- End of crawl.py ---
├── harvester.py
    --- Start of harvester.py ---
    import json
    import os
    import re
    import numpy as np
    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
    from sklearn.decomposition import LatentDirichletAllocation, NMF
    from sklearn.metrics.pairwise import cosine_similarity
    import matplotlib.pyplot as plt
    from wordcloud import WordCloud
    from collections import Counter
    import networkx as nx
    
    class Harvester:
        """
        Harvester extracts valuable patterns and insights from mimicked content,
        drawing nutrients and energy from it.
        """
        
        def __init__(self, input_dir=None, output_dir=None):
            """
            Initialize the Harvester.
            
            Args:
                input_dir (str): Directory containing mimicked content
                output_dir (str): Directory to save harvested insights
            """
            import os
            
            # Set default paths within the carnis_data directory structure
            if input_dir is None:
                input_dir = os.path.join('carnis_data', 'mimic')
            
            if output_dir is None:
                output_dir = os.path.join('carnis_data', 'harvester')
            
            self.input_dir = input_dir
            self.output_dir = output_dir
            self.mimicked_samples = []
            self.harvested_insights = {}
            
            # Create output directory
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
                
            # Create visualization subdirectory
            self.viz_dir = os.path.join(output_dir, 'visualizations')
            if not os.path.exists(self.viz_dir):
                os.makedirs(self.viz_dir)
        
        def load_mimicked_content(self):
            """Load mimicked content samples"""
            try:
                all_samples_file = os.path.join(self.input_dir, "all_mimicked_samples.json")
                if os.path.exists(all_samples_file):
                    with open(all_samples_file, 'r', encoding='utf-8') as f:
                        self.mimicked_samples = json.load(f)
                    print(f"Loaded {len(self.mimicked_samples)} mimicked content samples.")
                    return True
                else:
                    # Try loading individual sample files
                    sample_files = [f for f in os.listdir(self.input_dir) if f.startswith('mimicked_sample_') and f.endswith('.json')]
                    for file in sample_files:
                        with open(os.path.join(self.input_dir, file), 'r', encoding='utf-8') as f:
                            self.mimicked_samples.append(json.load(f))
                    
                    if self.mimicked_samples:
                        print(f"Loaded {len(self.mimicked_samples)} individual mimicked samples.")
                        return True
                    else:
                        print(f"No mimicked content found in {self.input_dir}")
                        return False
            except Exception as e:
                print(f"Error loading mimicked content: {e}")
                return False
        
        def extract_content_features(self):
            """Extract key linguistic features from mimicked content"""
            if not self.mimicked_samples:
                if not self.load_mimicked_content():
                    return {}
            
            # Collect all content texts
            content_texts = [sample.get('content', '') for sample in self.mimicked_samples]
            
            # Initialize feature extraction
            features = {
                'word_frequencies': Counter(),
                'phrase_patterns': Counter(),
                'sentiment_distribution': {},
                'topic_distribution': None,
                'conceptual_relationships': []
            }
            
            # Word frequencies (excluding common stopwords)
            vectorizer = CountVectorizer(stop_words='english', max_features=100)
            word_counts = vectorizer.fit_transform(content_texts)
            words = vectorizer.get_feature_names_out()
            word_freq = np.asarray(word_counts.sum(axis=0)).ravel()
            word_freq_dict = dict(zip(words, word_freq))
            features['word_frequencies'] = Counter(word_freq_dict)
            
            # Common phrases (n-grams)
            phrase_vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words='english', max_features=50)
            phrase_counts = phrase_vectorizer.fit_transform(content_texts)
            phrases = phrase_vectorizer.get_feature_names_out()
            phrase_freq = np.asarray(phrase_counts.sum(axis=0)).ravel()
            phrase_freq_dict = dict(zip(phrases, phrase_freq))
            features['phrase_patterns'] = Counter(phrase_freq_dict)
            
            # Topic modeling (Latent Dirichlet Allocation)
            tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')
            tfidf = tfidf_vectorizer.fit_transform(content_texts)
            
            # Find optimal number of topics
            num_topics = min(5, len(content_texts))
            
            try:
                lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
                lda.fit(tfidf)
                
                # Extract topics
                feature_names = tfidf_vectorizer.get_feature_names_out()
                topics = []
                for topic_idx, topic in enumerate(lda.components_):
                    top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]
                    topics.append({
                        'id': topic_idx,
                        'top_words': top_words,
                        'weight': float(np.sum(topic))
                    })
                
                features['topic_distribution'] = topics
            except Exception as e:
                print(f"Error in topic modeling: {e}")
            
            # Extract conceptual relationships
            for sample in self.mimicked_samples:
                # Extract entities and their relationships from content
                content = sample.get('content', '')
                # Simple pattern matching for relationship extraction
                # Looking for patterns like "X is Y" or "X does Y"
                relationship_patterns = [
                    (r'(\b\w+\b)\s+is\s+(\b\w+\b)', 'is'),
                    (r'(\b\w+\b)\s+are\s+(\b\w+\b)', 'are'),
                    (r'(\b\w+\b)\s+has\s+(\b\w+\b)', 'has'),
                    (r'(\b\w+\b)\s+contains\s+(\b\w+\b)', 'contains')
                ]
                
                for pattern, relation_type in relationship_patterns:
                    for match in re.finditer(pattern, content, re.IGNORECASE):
                        if match and len(match.groups()) >= 2:
                            source = match.group(1)
                            target = match.group(2)
                            features['conceptual_relationships'].append({
                                'source': source,
                                'relation': relation_type,
                                'target': target,
                                'sample_id': sample.get('id')
                            })
            
            return features
        
        def analyze_path_patterns(self):
            """Analyze patterns in the walk paths from mimicked content"""
            if not self.mimicked_samples:
                if not self.load_mimicked_content():
                    return {}
                    
            path_analytics = {
                'common_nodes': Counter(),
                'transitions': Counter(),
                'path_lengths': [],
                'connection_density': {}
            }
            
            # Extract all paths
            all_paths = []
            for sample in self.mimicked_samples:
                paths = sample.get('paths', [])
                all_paths.extend(paths)
            
            # Count node occurrences
            for path in all_paths:
                for node in path:
                    path_analytics['common_nodes'][node] += 1
                    
            # Count transitions
            transitions_dict = {}
            for path in all_paths:
                for i in range(len(path) - 1):
                    transition = (path[i], path[i+1])
                    transition_key = f"{path[i]}_to_{path[i+1]}"
                    if transition_key in transitions_dict:
                        transitions_dict[transition_key] += 1
                    else:
                        transitions_dict[transition_key] = 1
                        
            path_analytics['transitions'] = Counter(transitions_dict)
            
            # Path length distribution
            path_analytics['path_lengths'] = [len(path) for path in all_paths]
            
            # Connection density (how interconnected the concepts are)
            G = nx.DiGraph()
            for path in all_paths:
                for i in range(len(path) - 1):
                    G.add_edge(path[i], path[i+1])
            
            if len(G.nodes()) > 0:
                path_analytics['connection_density'] = {
                    'nodes': len(G.nodes()),
                    'edges': len(G.edges()),
                    'density': nx.density(G),
                    'average_clustering': nx.average_clustering(G.to_undirected())
                }
            
            return path_analytics
        
        def evaluate_content_quality(self):
            """Evaluate the quality of mimicked content"""
            if not self.mimicked_samples:
                if not self.load_mimicked_content():
                    return {}
            
            quality_metrics = {
                'coherence_scores': [],
                'complexity_metrics': [],
                'novelty_assessment': [],
                'overall_quality': {}
            }
            
            # Collect all content texts
            content_texts = [sample.get('content', '') for sample in self.mimicked_samples]
            
            # Measure coherence based on sentence flow
            for i, content in enumerate(content_texts):
                sentences = re.split(r'[.!?]', content)
                sentences = [s.strip() for s in sentences if s.strip()]
                
                if len(sentences) < 3:
                    coherence_score = 0.5  # Neutral score for very short content
                else:
                    # Count transition words as a proxy for coherence
                    transition_words = ['however', 'therefore', 'consequently', 'furthermore', 
                                       'moreover', 'thus', 'hence', 'accordingly', 'besides',
                                       'additionally', 'nonetheless', 'meanwhile', 'subsequently',
                                       'in conclusion', 'in summary']
                    
                    transition_count = sum(1 for word in transition_words 
                                         if re.search(r'\b' + word + r'\b', content, re.IGNORECASE))
                    
                    # Normalize by content length
                    coherence_score = min(1.0, transition_count / (len(content) / 500))
                    
                quality_metrics['coherence_scores'].append({
                    'sample_id': i + 1,
                    'score': coherence_score
                })
            
            # Measure complexity based on sentence length and word length
            for i, content in enumerate(content_texts):
                sentences = re.split(r'[.!?]', content)
                sentences = [s.strip() for s in sentences if s.strip()]
                
                if not sentences:
                    complexity_score = 0
                else:
                    # Average sentence length
                    avg_sentence_length = np.mean([len(s.split()) for s in sentences])
                    
                    # Average word length
                    words = content.split()
                    avg_word_length = np.mean([len(word) for word in words]) if words else 0
                    
                    # Combine into complexity score (normalized)
                    complexity_score = (avg_sentence_length / 20 + avg_word_length / 8) / 2
                
                quality_metrics['complexity_metrics'].append({
                    'sample_id': i + 1,
                    'complexity': min(1.0, complexity_score),
                    'avg_sentence_length': float(avg_sentence_length) if sentences else 0,
                    'avg_word_length': float(avg_word_length) if words else 0
                })
            
            # Assess novelty by comparing samples to each other
            if len(content_texts) > 1:
                vectorizer = TfidfVectorizer(stop_words='english')
                tfidf_matrix = vectorizer.fit_transform(content_texts)
                
                # Compute pairwise similarities
                pairwise_similarity = cosine_similarity(tfidf_matrix)
                
                # For each sample, calculate average similarity to others (lower is more novel)
                for i in range(len(content_texts)):
                    # Exclude self-similarity (diagonal)
                    similarities = [pairwise_similarity[i, j] for j in range(len(content_texts)) if i != j]
                    avg_similarity = np.mean(similarities) if similarities else 0
                    novelty_score = 1 - avg_similarity  # Convert similarity to novelty
                    
                    quality_metrics['novelty_assessment'].append({
                        'sample_id': i + 1,
                        'novelty_score': float(novelty_score),
                        'avg_similarity_to_others': float(avg_similarity)
                    })
            
            # Calculate overall quality metrics
            if quality_metrics['coherence_scores'] and quality_metrics['complexity_metrics'] and quality_metrics['novelty_assessment']:
                avg_coherence = np.mean([item['score'] for item in quality_metrics['coherence_scores']])
                avg_complexity = np.mean([item['complexity'] for item in quality_metrics['complexity_metrics']])
                avg_novelty = np.mean([item['novelty_score'] for item in quality_metrics['novelty_assessment']])
                
                # Combined quality score
                overall_quality = (avg_coherence + avg_complexity + avg_novelty) / 3
                
                quality_metrics['overall_quality'] = {
                    'score': float(overall_quality),
                    'avg_coherence': float(avg_coherence),
                    'avg_complexity': float(avg_complexity),
                    'avg_novelty': float(avg_novelty)
                }
            
            return quality_metrics
        
        def harvest(self):
            """Extract insights from mimicked content"""
            # Load content if not already loaded
            if not self.mimicked_samples:
                if not self.load_mimicked_content():
                    return None
            
            print("Harvesting insights from mimicked content...")
            
            # Extract various types of insights
            content_features = self.extract_content_features()
            path_patterns = self.analyze_path_patterns()
            quality_assessment = self.evaluate_content_quality()
            
            # Combine all insights
            self.harvested_insights = {
                'content_features': content_features,
                'path_patterns': path_patterns,
                'quality_assessment': quality_assessment,
                'meta': {
                    'sample_count': len(self.mimicked_samples),
                    'harvested_at': time.time(),
                    'source': self.input_dir
                }
            }
            
            # Create visualizations
            self.generate_visualizations()
            
            # Save insights to file
            self.save_insights()
            
            return self.harvested_insights
        
        def generate_visualizations(self):
            """Generate visualizations of the harvested insights"""
            insights = self.harvested_insights
            
            if not insights:
                print("No insights to visualize.")
                return
            
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            # 1. Word cloud from word frequencies
            word_freq = insights.get('content_features', {}).get('word_frequencies', {})
            if word_freq:
                plt.figure(figsize=(10, 8))
                wc = WordCloud(background_color="white", width=800, height=600, 
                              colormap='viridis', max_words=100)
                wc.generate_from_frequencies(word_freq)
                plt.imshow(wc, interpolation='bilinear')
                plt.axis("off")
                plt.title('Key Concepts in Mimicked Content')
                plt.savefig(os.path.join(self.viz_dir, 'word_cloud.png'), dpi=300, bbox_inches='tight')
                plt.close()
            
            # 2. Topic distribution
            topics = insights.get('content_features', {}).get('topic_distribution', [])
            if topics:
                plt.figure(figsize=(12, 6))
                topic_names = [', '.join(topic['top_words'][:3]) for topic in topics]
                topic_weights = [topic['weight'] for topic in topics]
                
                # Normalize weights
                if sum(topic_weights) > 0:
                    topic_weights = [w / sum(topic_weights) for w in topic_weights]
                
                plt.barh(topic_names, topic_weights, color='skyblue')
                plt.xlabel('Relative Weight')
                plt.title('Topic Distribution in Mimicked Content')
                plt.tight_layout()
                plt.savefig(os.path.join(self.viz_dir, 'topic_distribution.png'), dpi=300, bbox_inches='tight')
                plt.close()
            
            # 3. Quality metrics comparison
            quality = insights.get('quality_assessment', {}).get('overall_quality', {})
            if quality:
                categories = ['Coherence', 'Complexity', 'Novelty', 'Overall']
                scores = [quality.get('avg_coherence', 0), 
                         quality.get('avg_complexity', 0),
                         quality.get('avg_novelty', 0),
                         quality.get('score', 0)]
                
                plt.figure(figsize=(8, 6))
                plt.bar(categories, scores, color=['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3'])
                plt.ylim(0, 1)
                plt.ylabel('Score')
                plt.title('Content Quality Assessment')
                plt.savefig(os.path.join(self.viz_dir, 'quality_metrics.png'), dpi=300, bbox_inches='tight')
                plt.close()
            
            # 4. Concept relationships network
            relationships = insights.get('content_features', {}).get('conceptual_relationships', [])
            if relationships:
                G = nx.DiGraph()
                
                # Add edges for each relationship
                for rel in relationships:
                    source = rel['source'].lower()
                    target = rel['target'].lower()
                    relation = rel['relation']
                    
                    G.add_edge(source, target, label=relation)
                
                # Filter to keep the graph manageable (max 20 nodes)
                if len(G.nodes()) > 20:
                    # Keep nodes with highest degree
                    degrees = dict(G.degree())
                    top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:20]
                    top_node_names = [node for node, _ in top_nodes]
                    G = nx.subgraph(G, top_node_names)
                
                plt.figure(figsize=(10, 8))
                pos = nx.spring_layout(G, k=0.15, iterations=50)
                
                nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue', alpha=0.8)
                nx.draw_networkx_labels(G, pos, font_size=10)
                nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, arrows=True)
                
                plt.title("Conceptual Relationships Network")
                plt.axis('off')
                plt.tight_layout()
                plt.savefig(os.path.join(self.viz_dir, 'concept_network.png'), dpi=300, bbox_inches='tight')
                plt.close()
            
            print(f"Visualizations saved to {self.viz_dir}")
        
        def _ensure_json_serializable(self, obj):
            """Convert NumPy types and non-serializable objects to standard Python types for JSON serialization"""
            if isinstance(obj, dict):
                # Convert any non-string keys to strings (especially tuples)
                return {str(k) if not isinstance(k, (str, int, float, bool, type(None))) else k: 
                        self._ensure_json_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [self._ensure_json_serializable(item) for item in obj]
            elif isinstance(obj, (np.integer, np.int64, np.int32)):
                return int(obj)
            elif isinstance(obj, (float, np.float64, np.float32)):  # np.float removed
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return self._ensure_json_serializable(obj.tolist())
            elif isinstance(obj, tuple):
                # Convert tuple to string when it's likely to be used as a key
                if len(obj) == 2 and all(isinstance(x, str) for x in obj):
                    return f"{obj[0]}_to_{obj[1]}"
                else:
                    return tuple(self._ensure_json_serializable(item) for item in obj)
            elif hasattr(obj, 'item'):  # Handle any other numpy scalar types
                return obj.item()
            else:
                return obj
    
        def save_insights(self):
            """Save harvested insights to file"""
            if not self.harvested_insights:
                print("No insights to save.")
                return
                
            # Convert NumPy types to standard Python types
            serializable_insights = self._ensure_json_serializable(self.harvested_insights)
            
            output_file = os.path.join(self.output_dir, 'harvested_insights.json')
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_insights, f, indent=2)
                
            print(f"Harvested insights saved to {output_file}")
            
            # Save a summary file with key findings
            summary = self.generate_summary()
            summary_file = os.path.join(self.output_dir, 'insights_summary.txt')
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(summary)
                
            print(f"Summary saved to {summary_file}")
        
        def generate_summary(self):
            """Generate a textual summary of key insights"""
            insights = self.harvested_insights
            if not insights:
                return "No insights available."
                
            summary_parts = ["# HARVESTED INSIGHTS SUMMARY", ""]
            
            # Summary of content features
            summary_parts.append("## Content Analysis")
            
            # Top words
            word_freq = insights.get('content_features', {}).get('word_frequencies', {})
            if word_freq:
                top_words = [word for word, _ in word_freq.most_common(10)]
                summary_parts.append(f"Top concepts: {', '.join(top_words)}")
            
            # Topics
            topics = insights.get('content_features', {}).get('topic_distribution', [])
            if topics:
                summary_parts.append("\nIdentified topics:")
                for i, topic in enumerate(topics):
                    topic_terms = ', '.join(topic['top_words'][:5])
                    summary_parts.append(f"- Topic {i+1}: {topic_terms}")
            
            # Path patterns
            path_patterns = insights.get('path_patterns', {})
            if path_patterns:
                summary_parts.append("\n## Path Analysis")
                
                common_nodes = path_patterns.get('common_nodes', {})
                if common_nodes:
                    top_nodes = [node for node, _ in common_nodes.most_common(5)]
                    summary_parts.append(f"Central concepts: {', '.join(top_nodes)}")
                
                connection_density = path_patterns.get('connection_density', {})
                if connection_density:
                    density = connection_density.get('density', 0)
                    clustering = connection_density.get('average_clustering', 0)
                    summary_parts.append(f"Network density: {density:.3f}")
                    summary_parts.append(f"Average clustering: {clustering:.3f}")
            
            # Quality assessment
            quality = insights.get('quality_assessment', {}).get('overall_quality', {})
            if quality:
                summary_parts.append("\n## Quality Assessment")
                summary_parts.append(f"Overall quality score: {quality.get('score', 0):.2f}/1.00")
                summary_parts.append(f"Coherence: {quality.get('avg_coherence', 0):.2f}/1.00")
                summary_parts.append(f"Complexity: {quality.get('avg_complexity', 0):.2f}/1.00")
                summary_parts.append(f"Novelty: {quality.get('avg_novelty', 0):.2f}/1.00")
            
            # Recommendations
            summary_parts.append("\n## Recommendations")
            
            # Add recommendations based on insights
            if quality and quality.get('avg_coherence', 0) < 0.5:
                summary_parts.append("- Improve coherence by strengthening connections between concepts")
                
            if path_patterns and path_patterns.get('connection_density', {}).get('density', 1) < 0.2:
                summary_parts.append("- Increase connectivity between concepts to create a more robust knowledge structure")
                
            if quality and quality.get('avg_novelty', 0) < 0.5:
                summary_parts.append("- Introduce more diverse concepts to increase novelty")
                
            # Add generic recommendations if specific ones weren't added
            if len(summary_parts) < 3:
                summary_parts.append("- Continue to evolve the knowledge graph with new information sources")
                summary_parts.append("- Focus on strengthening relationships between key concepts")
                summary_parts.append("- Consider expanding into adjacent domains to increase system robustness")
            
            return "\n".join(summary_parts)
    
    import time
    
    if __name__ == "__main__":
        # Example usage with default paths that use the carnis_data directory structure
        harvester = Harvester()
        harvested_data = harvester.harvest()
    --- End of harvester.py ---
├── host.py
    --- Start of host.py ---
    import os
    import json
    import time
    from datetime import datetime
    import threading
    import importlib
    from flask import Flask, request, jsonify, render_template_string, send_from_directory
    import logging
    import argparse
    import yaml
    
    # Import components
    from crawl import Crawler
    from trimmings import Trimmings
    from meatsnake import Meatsnake
    from mimic import Mimic
    from harvester import Harvester
    
    class Host:
        """
        Host integrates and controls all components of the Carnis system,
        providing a unified interface and coordination - like a host organism
        that has been subsumed by the parasite and now serves its purposes.
        """
        
        def __init__(self, config_file='config.yaml'):
            """
            Initialize the Host controller.
            
            Args:
                config_file (str): Path to the configuration file
            """
            self.config_file = config_file
            self.config = self._load_config()
            self.components = {}
            self.state = {
                'status': 'initialized',
                'last_update': time.time(),
                'cycle_count': 0,
                'component_states': {},
                'current_task': None
            }
            
            # Data storage paths
            self.data_dir = self.config.get('data_dir', 'carnis_data')
            if not os.path.exists(self.data_dir):
                os.makedirs(self.data_dir)
            
            # Set up logging
            self._setup_logging()
            
            # Initialize component registry with default configurations
            self._init_component_registry()
                
            # Initialize API server if enabled
            self.api_server = None
            if self.config.get('api_enabled', True):
                self.api_thread = None
                self._init_api_server()
        
        def _load_config(self):
            """Load configuration from file"""
            try:
                if os.path.exists(self.config_file):
                    with open(self.config_file, 'r') as f:
                        loaded_config = yaml.safe_load(f)
                        # Handle the case where the file exists but is empty or invalid YAML
                        if loaded_config is None:
                            self.logger.warning(f"Config file {self.config_file} exists but is empty or invalid. Writing default configuration.")
                            # Write default config to the file
                            default_config = self._get_default_config()
                            with open(self.config_file, 'w') as f:
                                yaml.dump(default_config, f, default_flow_style=False)
                            return default_config
                        return loaded_config
                else:
                    # Create default configuration file
                    default_config = self._get_default_config()
                    os.makedirs(os.path.dirname(os.path.abspath(self.config_file)), exist_ok=True)
                    with open(self.config_file, 'w') as f:
                        yaml.dump(default_config, f, default_flow_style=False)
                    print(f"Created new configuration file at {self.config_file}")
                    return default_config
            except Exception as e:
                print(f"Error loading configuration: {e}")
                # Fallback to minimal config - make sure this ALWAYS returns a dictionary
                return {'data_dir': 'carnis_data', 'api_enabled': True}
                
        def _get_default_config(self):
            """Return the default configuration"""
            return {
                'data_dir': 'carnis_data',
                'api_enabled': True,
                'api_host': '127.0.0.1',
                'api_port': 5000,
                'log_level': 'INFO',
                'auto_cycle': False,
                'cycle_interval': 86400,  # 24 hours
                'components': {
                    'crawl': {
                        'max_pages': 50,
                        'delay': 1.5,
                        'seed_urls': [
                            "https://en.wikipedia.org/wiki/Artificial_intelligence",
                            "https://en.wikipedia.org/wiki/Biology",
                            "https://en.wikipedia.org/wiki/Biotechnology"
                        ]
                    },
                    'trimmings': {},
                    'meatsnake': {},
                    'mimic': {
                        'num_samples': 3
                    },
                    'harvester': {}
                }
            }
        
        def _setup_logging(self):
            """Configure logging for the system"""
            log_level = self.config.get('log_level', 'INFO')
            log_dir = os.path.join(self.data_dir, 'logs')
            if not os.path.exists(log_dir):
                os.makedirs(log_dir)
                
            log_file = os.path.join(log_dir, f'carnis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
            
            # Set up root logger
            logging.basicConfig(
                level=getattr(logging, log_level),
                format='%(asctime)s [%(levelname)s] %(message)s',
                handlers=[
                    logging.FileHandler(log_file),
                    logging.StreamHandler()
                ]
            )
            
            self.logger = logging.getLogger('host')
            self.logger.info("Host system initialized")
        
        def _init_component_registry(self):
            """Initialize the component registry with default configurations"""
            self.components = {
                'crawl': {
                    'class': Crawler,
                    'initialized': False,
                    'instance': None,
                    'config': self.config.get('components', {}).get('crawl', {})
                },
                'trimmings': {
                    'class': Trimmings,
                    'initialized': False,
                    'instance': None,
                    'config': self.config.get('components', {}).get('trimmings', {})
                },
                'meatsnake': {
                    'class': Meatsnake,
                    'initialized': False,
                    'instance': None,
                    'config': self.config.get('components', {}).get('meatsnake', {})
                },
                'mimic': {
                    'class': Mimic,
                    'initialized': False,
                    'instance': None,
                    'config': self.config.get('components', {}).get('mimic', {})
                },
                'harvester': {
                    'class': Harvester,
                    'initialized': False,
                    'instance': None,
                    'config': self.config.get('components', {}).get('harvester', {})
                }
            }
            
            # Update component states
            for name in self.components:
                self.state['component_states'][name] = 'registered'
        
        def _init_api_server(self):
            """Initialize the API server"""
            self.api_server = Flask(__name__)
            
            # Define API routes
            @self.api_server.route('/', methods=['GET'])
            def home():
                """Render simple web interface"""
                return render_template_string('''
                <!-- Replace the existing template_string in the home() function with this enhanced version -->
    <!DOCTYPE html>
    <html>
    <head>
        <title>Carnis Host Interface</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tom-select@2.2.2/dist/css/tom-select.bootstrap5.min.css">
        <style>
            body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; }
            .dashboard { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; }
            .card { border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin-bottom: 20px; }
            .card-header { font-weight: 600; }
            .component { transition: all 0.3s ease; }
            .component:hover { transform: translateY(-5px); }
            .component-running { border-left: 4px solid #17a2b8; animation: pulse 2s infinite; }
            .component-completed { border-left: 4px solid #28a745; }
            .component-failed { border-left: 4px solid #dc3545; }
            .status-badge { float: right; }
            .log-container { height: 300px; overflow-y: auto; background: #f8f9fa; border-radius: 4px; padding: 10px; font-family: monospace; }
            @keyframes pulse {
                0% { box-shadow: 0 0 0 0 rgba(23, 162, 184, 0.5); }
                70% { box-shadow: 0 0 0 10px rgba(23, 162, 184, 0); }
                100% { box-shadow: 0 0 0 0 rgba(23, 162, 184, 0); }
            }
            .graph-viz { width: 100%; height: 400px; border: 1px solid #ddd; border-radius: 4px; }
            .tab-content { padding: 20px 0; }
        </style>
    </head>
    <body>
        <div class="container mt-4">
            <header class="mb-4">
                <h1 class="display-4">Carnis <small class="text-muted">Host Interface</small></h1>
                <p class="lead">Monitoring and control panel for the Carnis system</p>
            </header>
    
            <ul class="nav nav-tabs" id="myTab" role="tablist">
                <li class="nav-item" role="presentation">
                    <button class="nav-link active" id="dashboard-tab" data-bs-toggle="tab" data-bs-target="#dashboard" type="button">Dashboard</button>
                </li>
                <li class="nav-item" role="presentation">
                    <button class="nav-link" id="logs-tab" data-bs-toggle="tab" data-bs-target="#logs" type="button">Logs</button>
                </li>
                <li class="nav-item" role="presentation">
                    <button class="nav-link" id="config-tab" data-bs-toggle="tab" data-bs-target="#config" type="button">Configuration</button>
                </li>
                <li class="nav-item" role="presentation">
                    <button class="nav-link" id="visualize-tab" data-bs-toggle="tab" data-bs-target="#visualize" type="button">Visualization</button>
                </li>
            </ul>
    
            <div class="tab-content" id="myTabContent">
                <!-- Dashboard Tab -->
                <div class="tab-pane fade show active" id="dashboard" role="tabpanel">
                    <div class="dashboard">
                        <div class="card">
                            <div class="card-header bg-primary text-white">
                                System Status
                            </div>
                            <div class="card-body">
                                <div class="d-flex justify-content-between align-items-center mb-3">
                                    <h5>Current Status:</h5>
                                    <span id="currentStatus" class="badge bg-secondary">Loading...</span>
                                </div>
                                <div class="d-flex justify-content-between align-items-center mb-3">
                                    <h5>Current Task:</h5>
                                    <span id="currentTask" class="badge bg-info">None</span>
                                </div>
                                <div class="d-flex justify-content-between align-items-center mb-3">
                                    <h5>Cycle Count:</h5>
                                    <span id="cycleCount">0</span>
                                </div>
                                <div class="d-flex justify-content-between align-items-center mb-3">
                                    <h5>Last Update:</h5>
                                    <span id="lastUpdate">-</span>
                                </div>
                            </div>
                        </div>
    
                        <div class="card">
                            <div class="card-header bg-success text-white">
                                Quick Controls
                            </div>
                            <div class="card-body">
                                <button id="runCycleBtn" class="btn btn-primary btn-lg mb-3 w-100">Run Full Cycle</button>
                                
                                <div class="mb-3">
                                    <label for="componentSelect" class="form-label">Run Component:</label>
                                    <select id="componentSelect" class="form-select">
                                        <option value="crawl">Crawl</option>
                                        <option value="trimmings">Trimmings</option>
                                        <option value="meatsnake">Meatsnake</option>
                                        <option value="mimic">Mimic</option>
                                        <option value="harvester">Harvester</option>
                                    </select>
                                </div>
                                <button id="runComponentBtn" class="btn btn-success w-100">Run Selected Component</button>
                            </div>
                        </div>
                    </div>
                    
                    <div class="row mt-4">
                        <div class="col-12">
                            <div class="card">
                                <div class="card-header bg-dark text-white">
                                    Component Status
                                </div>
                                <div class="card-body">
                                    <div id="componentStatusList"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
    
                <!-- Components Tab -->
                <div class="tab-pane fade" id="components" role="tabpanel">
                    <div class="accordion" id="componentsAccordion">
                        <!-- Crawl -->
                        <div class="accordion-item">
                            <h2 class="accordion-header">
                                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#crawler">
                                    Crawl <span id="crawlerStatus" class="badge bg-secondary ms-2">Unknown</span>
                                </button>
                            </h2>
                            <div id="crawler" class="accordion-collapse collapse show">
                                <div class="accordion-body">
                                    <div class="mb-3">
                                        <label class="form-label">Seed URLs:</label>
                                        <select id="seedUrls" multiple placeholder="Add seed URLs..."></select>
                                    </div>
                                    <div class="row">
                                        <div class="col-md-6">
                                            <div class="mb-3">
                                                <label for="maxPages" class="form-label">Max Pages:</label>
                                                <input type="number" class="form-control" id="maxPages" value="50">
                                            </div>
                                        </div>
                                        <div class="col-md-6">
                                            <div class="mb-3">
                                                <label for="crawlDelay" class="form-label">Delay (seconds):</label>
                                                <input type="number" class="form-control" id="crawlDelay" step="0.1" value="1.5">
                                            </div>
                                        </div>
                                    </div>
                                    <button id="saveCrawlConfig" class="btn btn-primary">Save Configuration</button>
                                    <button id="runCrawl" class="btn btn-success ms-2">Run Crawl</button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Other components follow the same pattern -->
                    </div>
                </div>
    
                <!-- Logs Tab -->
                <div class="tab-pane fade" id="logs" role="tabpanel">
                    <div class="card">
                        <div class="card-header bg-dark text-white d-flex justify-content-between align-items-center">
                            <span>System Logs</span>
                            <button id="refreshLogs" class="btn btn-sm btn-outline-light">Refresh</button>
                        </div>
                        <div class="card-body">
                            <div class="mb-3">
                                <label for="logLevel" class="form-label">Log Level:</label>
                                <select id="logLevel" class="form-select form-select-sm w-auto">
                                    <option value="all">All</option>
                                    <option value="info">Info & Above</option>
                                    <option value="warning">Warning & Above</option>
                                    <option value="error">Errors Only</option>
                                </select>
                            </div>
                            <div class="log-container" id="logContent">Loading logs...</div>
                        </div>
                    </div>
                </div>
    
                <!-- Configuration Tab -->
                <div class="tab-pane fade" id="config" role="tabpanel">
                    <div class="card">
                        <div class="card-header bg-dark text-white">
                            System Configuration
                        </div>
                        <div class="card-body">
                            <div class="mb-3">
                                <label for="configEditor" class="form-label">Configuration YAML:</label>
                                <textarea id="configEditor" class="form-control" style="height: 400px; font-family: monospace;"></textarea>
                            </div>
                            <button id="saveConfig" class="btn btn-primary">Save Configuration</button>
                            <button id="reloadConfig" class="btn btn-secondary ms-2">Reload</button>
                        </div>
                    </div>
                </div>
    
                <!-- Visualization Tab -->
                <div class="tab-pane fade" id="visualize" role="tabpanel">
                    <ul class="nav nav-pills mb-3">
                        <li class="nav-item">
                            <button class="nav-link active" data-bs-toggle="pill" data-bs-target="#harvesterViz">Harvester</button>
                        </li>
                        <li class="nav-item">
                            <button class="nav-link" data-bs-toggle="pill" data-bs-target="#mimicViz">Mimic Paths</button>
                        </li>
                    </ul>
                    <div class="tab-content">
                        <div class="tab-pane fade show active" id="harvesterViz">
                            <div class="card mb-4">
                                <div class="card-header">Harvested Insights</div>
                                <div class="card-body">
                                    <div class="row">
                                        <div class="col-md-6 mb-4">
                                            <div class="card h-100">
                                                <div class="card-header">Word Cloud</div>
                                                <div class="card-body text-center">
                                                    <img id="wordCloudImg" src="" alt="Word Cloud" class="img-fluid" style="max-height: 300px;">
                                                </div>
                                            </div>
                                        </div>
                                        <div class="col-md-6 mb-4">
                                            <div class="card h-100">
                                                <div class="card-header">Quality Metrics</div>
                                                <div class="card-body text-center">
                                                    <img id="qualityMetricsImg" src="" alt="Quality Metrics" class="img-fluid" style="max-height: 300px;">
                                                </div>
                                            </div>
                                        </div>
                                        <div class="col-md-6 mb-4">
                                            <div class="card h-100">
                                                <div class="card-header">Topic Distribution</div>
                                                <div class="card-body text-center">
                                                    <img id="topicDistributionImg" src="" alt="Topic Distribution" class="img-fluid" style="max-height: 300px;">
                                                </div>
                                            </div>
                                        </div>
                                        <div class="col-md-6 mb-4">
                                            <div class="card h-100">
                                                <div class="card-header">Concept Network</div>
                                                <div class="card-body text-center">
                                                    <img id="conceptNetworkImg" src="" alt="Concept Network" class="img-fluid" style="max-height: 300px;">
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="mt-3">
                                        <h5>Insights Summary</h5>
                                        <div id="insightsSummary" class="p-3 bg-light">Loading insights...</div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="tab-pane fade" id="mimicViz">
                            <div class="card">
                                <div class="card-header">Mimic Paths Visualization</div>
                                <div class="card-body">
                                    <div id="mimicSamples" class="mb-3"></div>
                                    <div id="mimicPaths"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
    
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/tom-select@2.2.2/dist/js/tom-select.complete.min.js"></script>
        <script>
            // Initialize TomSelect for seed URLs
            let seedUrlsSelect = new TomSelect('#seedUrls', {
                create: true,
                createOnBlur: true,
                persist: false,
                placeholder: 'Add seed URLs...'
            });
            
            // Function to update status
            function updateStatus() {
                fetch('/api/status')
                    .then(response => response.json())
                    .then(data => {
                        // Update dashboard elements
                        document.getElementById('currentStatus').textContent = data.status;
                        document.getElementById('currentStatus').className = 'badge ' + getStatusClass(data.status);
                        document.getElementById('currentTask').textContent = data.current_task || 'None';
                        document.getElementById('cycleCount').textContent = data.cycle_count;
                        document.getElementById('lastUpdate').textContent = new Date(data.last_update * 1000).toLocaleString();
                        
                        // Replace the existing component status list code
                        const componentList = document.getElementById('componentStatusList');
                        componentList.innerHTML = '';
    
                        // Define the hierarchical order
                        const componentOrder = ['crawl', 'trimmings', 'meatsnake', 'mimic', 'harvester'];
                            
                        // Loop through components in the defined order
                        for (const component of componentOrder) {
                            if (component in data.component_states) {
                                const status = data.component_states[component];
                                const statusClass = getComponentStatusClass(status);
                                const div = document.createElement('div');
                                div.className = `component p-3 mb-2 ${statusClass}`;
                                div.innerHTML = `
                                    <h5>${capitalizeFirstLetter(component)} 
                                        <span class="status-badge badge ${getStatusBadgeClass(status)}">${status}</span>
                                    </h5>
                                    <div class="progress mb-2">
                                        <div class="progress-bar ${getProgressBarClass(status)}" 
                                            role="progressbar" 
                                            style="width: ${getProgressWidth(status)}%" 
                                            aria-valuenow="${getProgressWidth(status)}" 
                                            aria-valuemin="0" 
                                            aria-valuemax="100"></div>
                                    </div>
                                `;
                                componentList.appendChild(div);
                                
                                // Also update the status on the components tab
                                const compStatus = document.getElementById(`${component}Status`);
                                if (compStatus) {
                                    compStatus.textContent = status;
                                    compStatus.className = 'badge ' + getStatusBadgeClass(status) + ' ms-2';
                                }
                            }
                        }
                        
                        // Populate config editor if it's empty
                        const configEditor = document.getElementById('configEditor');
                        if (configEditor.value === '') {
                            fetch('/api/config')
                                .then(response => response.text())
                                .then(config => {
                                    configEditor.value = config;
                                });
                        }
                    })
                    .catch(error => {
                        console.error('Error fetching status:', error);
                    });
            }
            
            // Helper functions for styling
            function getStatusClass(status) {
                switch (status) {
                    case 'processing': return 'bg-info';
                    case 'running_cycle': return 'bg-primary';
                    case 'idle': return 'bg-success';
                    case 'error': return 'bg-danger';
                    default: return 'bg-secondary';
                }
            }
            
            function getComponentStatusClass(status) {
                if (status.includes('running')) {
                    return 'component-running';
                } else if (status === 'completed') {
                    return 'component-completed';
                } else if (status.includes('failed') || status.includes('error')) {
                    return 'component-failed';
                } else {
                    return '';
                }
            }
            
            function getStatusBadgeClass(status) {
                if (status.includes('running')) {
                    return 'bg-info';
                } else if (status === 'completed') {
                    return 'bg-success';
                } else if (status.includes('failed') || status.includes('error')) {
                    return 'bg-danger';
                } else if (status === 'initialized') {
                    return 'bg-primary';
                } else {
                    return 'bg-secondary';
                }
            }
            
            function getProgressBarClass(status) {
                if (status.includes('running')) {
                    return 'progress-bar-striped progress-bar-animated bg-info';
                } else if (status === 'completed') {
                    return 'bg-success';
                } else if (status.includes('failed') || status.includes('error')) {
                    return 'bg-danger';
                } else {
                    return 'bg-secondary';
                }
            }
            
            function getProgressWidth(status) {
                if (status === 'registered') {
                    return 0;
                } else if (status === 'initialized') {
                    return 25;
                } else if (status.includes('running')) {
                    return 75;
                } else if (status === 'completed') {
                    return 100;
                } else if (status.includes('failed') || status.includes('error')) {
                    return 100;
                } else {
                    return 50;
                }
            }
            
            function capitalizeFirstLetter(string) {
                return string.charAt(0).toUpperCase() + string.slice(1);
            }
            
            // Event listeners
            document.getElementById('runCycleBtn').addEventListener('click', function() {
                fetch('/api/run-cycle', { method: 'POST' })
                    .then(response => response.json())
                    .then(data => {
                        alert(`Cycle initiated: ${data.message}`);
                        updateStatus();
                    })
                    .catch(error => {
                        console.error('Error starting cycle:', error);
                    });
            });
            
            document.getElementById('runComponentBtn').addEventListener('click', function() {
                const component = document.getElementById('componentSelect').value;
                fetch(`/api/run-component/${component}`, { method: 'POST' })
                    .then(response => response.json())
                    .then(data => {
                        alert(`Component ${component} initiated: ${data.message}`);
                        updateStatus();
                    })
                    .catch(error => {
                        console.error(`Error starting component ${component}:`, error);
                    });
            });
            
            document.getElementById('refreshLogs').addEventListener('click', function() {
                fetch('/api/logs')
                    .then(response => response.text())
                    .then(logs => {
                        document.getElementById('logContent').innerHTML = logs;
                    })
                    .catch(error => {
                        console.error('Error fetching logs:', error);
                    });
            });
            
            document.getElementById('saveConfig').addEventListener('click', function() {
                const config = document.getElementById('configEditor').value;
                fetch('/api/config', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/yaml' },
                    body: config
                })
                    .then(response => response.json())
                    .then(data => {
                        alert(`Configuration ${data.success ? 'saved' : 'failed to save'}: ${data.message}`);
                    })
                    .catch(error => {
                        console.error('Error saving config:', error);
                    });
            });
            
            document.getElementById('reloadConfig').addEventListener('click', function() {
                fetch('/api/config')
                    .then(response => response.text())
                    .then(config => {
                        document.getElementById('configEditor').value = config;
                    })
                    .catch(error => {
                        console.error('Error loading config:', error);
                    });
            });
            
            // Load mimic samples
            function loadMimicSamples() {
                fetch('/api/mimic-samples')
                    .then(response => response.json())
                    .then(samples => {
                        const samplesList = document.getElementById('mimicSamples');
                        samplesList.innerHTML = '';
                        
                        if (samples.length === 0) {
                            samplesList.innerHTML = '<div class="alert alert-info">No mimic samples available.</div>';
                            return;
                        }
                        
                        const select = document.createElement('select');
                        select.className = 'form-select mb-3';
                        select.id = 'mimicSampleSelect';
                        
                        samples.forEach((sample, index) => {
                            const option = document.createElement('option');
                            option.value = index;
                            option.textContent = `Sample ${sample.id}: ${sample.seed_concepts.join(', ').substring(0, 50)}...`;
                            select.appendChild(option);
                        });
                        
                        samplesList.appendChild(select);
                        select.addEventListener('change', function() {
                            displayMimicSample(samples[this.value]);
                        });
                        
                        if (samples.length > 0) {
                            displayMimicSample(samples[0]);
                        }
                    })
                    .catch(error => {
                        console.error('Error loading mimic samples:', error);
                    });
            }
            
            function displayMimicSample(sample) {
                const pathsContainer = document.getElementById('mimicPaths');
                pathsContainer.innerHTML = '';
                
                const card = document.createElement('div');
                card.className = 'card';
                card.innerHTML = `
                    <div class="card-header">
                        <h5>Sample ${sample.id}</h5>
                    </div>
                    <div class="card-body">
                        <h6>Seed Concepts:</h6>
                        <ul class="list-group mb-3">
                            ${sample.seed_concepts.map(concept => `<li class="list-group-item">${concept}</li>`).join('')}
                        </ul>
                        
                        <ul class="nav nav-tabs mb-3" id="sampleTabs" role="tablist">
                            <li class="nav-item" role="presentation">
                                <button class="nav-link active" id="details-tab" data-bs-toggle="tab" 
                                    data-bs-target="#details-content" type="button">Details</button>
                            </li>
                            <li class="nav-item" role="presentation">
                                <button class="nav-link" id="paths-tab" data-bs-toggle="tab" 
                                    data-bs-target="#paths-viz" type="button">Path Visualization</button>
                            </li>
                        </ul>
                        
                        <div class="tab-content">
                            <div class="tab-pane fade show active" id="details-content">
                                <h6>Prompt:</h6>
                                <div class="p-3 mb-3 bg-light">${sample.prompt}</div>
                                <h6>Generated Content:</h6>
                                <div class="p-3 mb-3 bg-light">${sample.content}</div>
                                <h6>Paths (text):</h6>
                                <ul class="list-group">
                                    ${sample.paths.map(path => `
                                        <li class="list-group-item">
                                            ${path.join(' → ')}
                                        </li>
                                    `).join('')}
                                </ul>
                            </div>
                            <div class="tab-pane fade" id="paths-viz">
                                <div class="mb-3">
                                    <div id="pathVisualization" style="width: 100%; height: 500px; border: 1px solid #ddd; border-radius: 4px;"></div>
                                </div>
                                <div class="small text-muted">Note: The visualization shows the conceptual paths traversed during mimicry.</div>
                            </div>
                        </div>
                    </div>
                `;
                
                pathsContainer.appendChild(card);
                
                // Initialize visualization when the paths tab is clicked
                document.getElementById('paths-tab').addEventListener('click', () => {
                    renderPathVisualization(sample);
                });
            }
    
            function renderPathVisualization(sample) {
                // This function will create a visualization of the paths
                const container = document.getElementById('pathVisualization');
                
                // If we need D3 or another visualization library, we can load it dynamically
                if (!window.d3) {
                    const script = document.createElement('script');
                    script.src = 'https://d3js.org/d3.v7.min.js';
                    script.onload = () => createVisualization(container, sample);
                    document.head.appendChild(script);
                } else {
                    createVisualization(container, sample);
                }
            }
    
                    function createVisualization(container, sample) {
                // Clear any existing visualization
                container.innerHTML = '';
                
                // Basic SVG setup
                const width = container.clientWidth;
                const height = container.clientHeight;
                
                const svg = d3.select(container)
                    .append('svg')
                    .attr('width', width)
                    .attr('height', height)
                    .append('g')
                    .attr('transform', 'translate(40,20)');
                    
                // Create a unique list of all concepts in all paths
                const allNodes = new Set();
                sample.paths.forEach(path => {
                    path.forEach(node => allNodes.add(node));
                });
                
                const nodes = Array.from(allNodes).map(name => ({ id: name, name }));
                
                // Create links from the paths
                const links = [];
                sample.paths.forEach(path => {
                    for (let i = 0; i < path.length - 1; i++) {
                        links.push({
                            source: path[i],
                            target: path[i + 1],
                            value: 1
                        });
                    }
                });
                
                // Combine duplicate links and increase their value
                const combinedLinks = {};
                links.forEach(link => {
                    const key = `${link.source}|${link.target}`;
                    if (combinedLinks[key]) {
                        combinedLinks[key].value += 1;
                    } else {
                        combinedLinks[key] = link;
                    }
                });
                
                const processedLinks = Object.values(combinedLinks);
                
                // Create a force-directed graph
                const simulation = d3.forceSimulation(nodes)
                    .force("link", d3.forceLink(processedLinks).id(d => d.id).distance(100))
                    .force("charge", d3.forceManyBody().strength(-300))
                    .force("center", d3.forceCenter(width / 2, height / 2))
                    .force("collision", d3.forceCollide().radius(50));
                
                // Draw the links
                const link = svg.append("g")
                    .selectAll("line")
                    .data(processedLinks)
                    .enter().append("line")
                    .attr("stroke-width", d => Math.sqrt(d.value) * 2)
                    .attr("stroke", "#999")
                    .attr("stroke-opacity", 0.6);
                
                // Create node groups
                const node = svg.append("g")
                    .selectAll(".node")
                    .data(nodes)
                    .enter().append("g")
                    .attr("class", "node")
                    .call(d3.drag()
                        .on("start", dragstarted)
                        .on("drag", dragged)
                        .on("end", dragended));
                
                // Add circles to nodes
                node.append("circle")
                    .attr("r", 10)
                    .attr("fill", (d) => {
                        if (sample.seed_concepts.includes(d.id)) {
                            return "#ff7f0e"; // orange for seed concepts
                        }
                        return "#1f77b4"; // default blue
                    });
                
                // Add labels to nodes
                node.append("text")
                    .attr("dx", 12)
                    .attr("dy", ".35em")
                    .text(d => d.name);
                
                // Update positions on simulation tick
                simulation.on("tick", () => {
                    link
                        .attr("x1", d => d.source.x)
                        .attr("y1", d => d.source.y)
                        .attr("x2", d => d.target.x)
                        .attr("y2", d => d.target.y);
                    
                    node
                        .attr("transform", d => `translate(${d.x},${d.y})`);
                });
                
                // Drag functions for interactive movement
                function dragstarted(event) {
                    if (!event.active) simulation.alphaTarget(0.3).restart();
                    event.subject.fx = event.subject.x;
                    event.subject.fy = event.subject.y;
                }
                
                function dragged(event) {
                    event.subject.fx = event.x;
                    event.subject.fy = event.y;
                }
                
                function dragended(event) {
                    if (!event.active) simulation.alphaTarget(0);
                    event.subject.fx = null;
                    event.subject.fy = null;
                }
                
                // Add legend
                const legend = svg.append("g")
                    .attr("transform", "translate(20,20)");
                    
                legend.append("circle")
                    .attr("r", 6)
                    .attr("fill", "#ff7f0e")
                    .attr("cx", 0)
                    .attr("cy", 0);
                    
                legend.append("text")
                    .attr("x", 15)
                    .attr("y", 4)
                    .text("Seed Concept");
                    
                legend.append("circle")
                    .attr("r", 6)
                    .attr("fill", "#1f77b4")
                    .attr("cx", 0)
                    .attr("cy", 25);
                    
                legend.append("text")
                    .attr("x", 15)
                    .attr("y", 29)
                    .text("Path Node");
                    
                // Add zoom capabilities
                const zoom = d3.zoom()
                    .scaleExtent([0.1, 4])
                    .on("zoom", (event) => {
                        svg.attr("transform", event.transform);
                    });
                    
                d3.select(container).select("svg")
                    .call(zoom);
            }
                                        
            // Load harvester visualizations
            function loadHarvesterVisualizations() {
                // Load images
                document.getElementById('wordCloudImg').src = '/carnis_data/harvester/visualizations/word_cloud.png?t=' + new Date().getTime();
                document.getElementById('qualityMetricsImg').src = '/carnis_data/harvester/visualizations/quality_metrics.png?t=' + new Date().getTime();
                document.getElementById('topicDistributionImg').src = '/carnis_data/harvester/visualizations/topic_distribution.png?t=' + new Date().getTime();
                document.getElementById('conceptNetworkImg').src = '/carnis_data/harvester/visualizations/concept_network.png?t=' + new Date().getTime();
                
                // Load insights summary text
                fetch('/api/harvester-insights')
                    .then(response => response.text())
                    .then(text => {
                        document.getElementById('insightsSummary').innerHTML = formatInsightsSummary(text);
                    })
                    .catch(error => {
                        document.getElementById('insightsSummary').innerHTML = 
                            '<div class="alert alert-warning">Failed to load insights summary: ' + error.message + '</div>';
                    });
            }
            
            // Format insights summary with better styling
            function formatInsightsSummary(text) {
                if (!text) return '<em>No insights available.</em>';
                
                // Split by line breaks and format as html
                const lines = text.split('\\n');
                let html = '';
                
                for (const line of lines) {
                    if (line.trim() === '') continue;
                    
                    if (line.startsWith('-')) {
                        // This is a bullet point
                        html += '<li>' + line.substring(1).trim() + '</li>';
                    } else if (line.startsWith('#')) {
                        // This is a header
                        html += '</ul><h6>' + line.substring(1).trim() + '</h6><ul>';
                    } else {
                        // Regular paragraph
                        html += '<p>' + line + '</p>';
                    }
                }
                
                // Wrap bullet points in ul tags
                if (html.includes('<li>')) {
                    html = '<ul>' + html + '</ul>';
                    // Clean up any empty ul tags
                    html = html.replace('<ul></ul>', '');
                }
                
                return html;
            }
            
            // Initialize the interface
            document.addEventListener('DOMContentLoaded', function() {
                updateStatus();
                setInterval(updateStatus, 5000);
                
                // Initialize the components tab
                fetch('/api/status')
                    .then(response => response.json())
                    .then(data => {
                        // Populate crawl configuration
                        if (data.components && data.components.crawl) {
                            const crawlerConfig = data.components.crawl.config;
                            if (crawlerConfig.seed_urls) {
                                seedUrlsSelect.clear();
                                crawlerConfig.seed_urls.forEach(url => {
                                    seedUrlsSelect.addOption({text: url, value: url});
                                    seedUrlsSelect.addItem(url);
                                });
                            }
                            
                            document.getElementById('maxPages').value = crawlerConfig.max_pages || 50;
                            document.getElementById('crawlDelay').value = crawlerConfig.delay || 1.5;
                        }
                    });
                    
                // Load logs
                document.getElementById('refreshLogs').click();
                
                // Load visualizations when tabs are clicked
                document.getElementById('visualize-tab').addEventListener('click', function() {
                    // Default to showing the harvester tab first
                    document.querySelector('[data-bs-target="#harvesterViz"]').click();
                    loadHarvesterVisualizations();
                });
                
                document.querySelector('[data-bs-target="#mimicViz"]').addEventListener('click', loadMimicSamples);
                document.querySelector('[data-bs-target="#harvesterViz"]').addEventListener('click', loadHarvesterVisualizations);
            });
        </script>
    </body>
    </html>
                ''')
            
            @self.api_server.route('/api/status', methods=['GET'])
            def get_status():
                """Get the current system status"""
                # Update the state before returning
                self.state['last_update'] = time.time()
                return jsonify(self.state)
            
            @self.api_server.route('/api/run-cycle', methods=['POST'])
            def run_cycle_endpoint():
                """API endpoint to start a full processing cycle"""
                # Run in a separate thread to not block the API
                threading.Thread(target=self.run_cycle).start()
                return jsonify({'status': 'success', 'message': 'Processing cycle started'})
            
            @self.api_server.route('/api/run-component/<component>', methods=['POST'])
            def run_component_endpoint(component):
                """API endpoint to run a specific component"""
                if component not in self.components:
                    return jsonify({'status': 'error', 'message': f'Component {component} not found'}), 404
                    
                # Run in a separate thread
                threading.Thread(target=self.run_component, args=(component,)).start()
                return jsonify({'status': 'success', 'message': f'Component {component} started'})
            
            @self.api_server.route('/api/config', methods=['GET'])
            def get_config():
                """Get the current configuration YAML"""
                try:
                    with open(self.config_file, 'r') as f:
                        return f.read(), 200, {'Content-Type': 'application/yaml'}
                except Exception as e:
                    return jsonify({'error': str(e)}), 500
    
            @self.api_server.route('/api/config', methods=['POST'])
            def update_config():
                """Update the configuration"""
                try:
                    config_yaml = request.data.decode('utf-8')
                    # Validate YAML format
                    new_config = yaml.safe_load(config_yaml)
                    
                    # Write to file
                    with open(self.config_file, 'w') as f:
                        f.write(config_yaml)
                        
                    # Reload config
                    self.config = self._load_config()
                    self._init_component_registry()
                    
                    return jsonify({'success': True, 'message': 'Configuration updated successfully'})
                except Exception as e:
                    return jsonify({'success': False, 'message': f'Error updating configuration: {str(e)}'}), 400
    
            @self.api_server.route('/api/logs', methods=['GET'])
            def get_logs():
                """Get system logs"""
                log_level = request.args.get('level', 'all')
                log_file = os.path.join(self.data_dir, 'logs', sorted(os.listdir(os.path.join(self.data_dir, 'logs')))[-1])
                
                try:
                    with open(log_file, 'r') as f:
                        logs = f.readlines()
                        
                    # Filter logs by level if needed
                    if log_level != 'all':
                        level_map = {
                            'debug': ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                            'info': ['INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                            'warning': ['WARNING', 'ERROR', 'CRITICAL'],
                            'error': ['ERROR', 'CRITICAL']
                        }
                        logs = [log for log in logs if any(level in log for level in level_map.get(log_level.lower(), []))]
                    
                    # Format logs for HTML display
                    formatted_logs = []
                    for log in logs:
                        log_class = 'text-muted'
                        if 'ERROR' in log or 'CRITICAL' in log:
                            log_class = 'text-danger'
                        elif 'WARNING' in log:
                            log_class = 'text-warning'
                        elif 'INFO' in log:
                            log_class = 'text-info'
                            
                        formatted_logs.append(f'<div class="{log_class}">{log}</div>')
                    
                    return ''.join(formatted_logs)
                except Exception as e:
                    return f'<div class="text-danger">Error reading logs: {str(e)}</div>'
    
            @self.api_server.route('/api/mimic-samples', methods=['GET'])
            def get_mimic_samples():
                """Get all mimic samples"""
                try:
                    mimic_dir = os.path.join(self.data_dir, 'mimic')
                    all_samples_file = os.path.join(mimic_dir, 'all_mimicked_samples.json')
                    
                    if not os.path.exists(all_samples_file):
                        return jsonify([])
                        
                    with open(all_samples_file, 'r', encoding='utf-8') as f:
                        samples = json.load(f)
                        # Fix: The file contains a direct list of samples, not a dict with 'samples' key
                        if isinstance(samples, list):
                            return jsonify(samples)
                        # Handle the case where it might be a dict with a 'samples' key
                        elif isinstance(samples, dict) and 'samples' in samples:
                            return jsonify(samples['samples'])
                        # Return an empty list as fallback
                        else:
                            return jsonify([])
                except Exception as e:
                    self.logger.error(f"Error getting mimic samples: {e}")
                    return jsonify([])
                
            
            @self.api_server.route('/api/harvester-insights', methods=['GET'])
            def get_harvester_insights():
                """Get harvester insights summary"""
                try:
                    insights_file = os.path.join(self.data_dir, 'harvester', 'insights_summary.txt')
                    
                    if not os.path.exists(insights_file):
                        return "No insights summary available yet. Run the harvester component first.", 200
                        
                    with open(insights_file, 'r', encoding='utf-8') as f:
                        insights_text = f.read()
                        
                    return insights_text, 200
                except Exception as e:
                    self.logger.error(f"Error getting harvester insights: {e}")
                    return f"Error loading insights: {str(e)}", 500
    
            # Serve static files from the data directory
            @self.api_server.route('/carnis_data/<path:path>')
            def serve_static(path):
                """Serve static files from the data directory"""
                # First try to serve from data_dir
                if os.path.exists(os.path.join(self.data_dir, path)):
                    return send_from_directory(self.data_dir, path)
                
                # Create directories for static files if they don't exist
                static_dirs = ['meatsnake', 'mimic', 'harvester']
                for dir_name in static_dirs:
                    full_dir = os.path.join(self.data_dir, dir_name)
                    os.makedirs(full_dir, exist_ok=True)
                    
                return send_from_directory(self.data_dir, path)
    
            @self.api_server.route('/carnis_data/<path:path>')
            def serve_data_files(path):
                """Serve files directly from the data directory structure"""
                return send_from_directory(self.data_dir, path)
            
            # Configuration for the API server
            self.api_host = self.config.get('api_host', '127.0.0.1')
            self.api_port = self.config.get('api_port', 5000)
    
            @self.api_server.route('/api/component-config/<component>', methods=['POST'])
            def update_component_config_endpoint(component):
                """API endpoint to update component configuration"""
                if component not in self.components:
                    return jsonify({'success': False, 'message': f'Component {component} not found'}), 404
                    
                try:
                    config_data = request.json
                    success, message = self.update_component_config(component, config_data)
                    return jsonify({'success': success, 'message': message})
                except Exception as e:
                    return jsonify({'success': False, 'message': f'Error: {str(e)}'}), 400
        
        def start_api_server(self):
            """Start the API server in a separate thread"""
            if not self.api_server:
                self._init_api_server()
                
            if self.api_thread is None or not self.api_thread.is_alive():
                self.api_thread = threading.Thread(
                    target=self.api_server.run,
                    kwargs={
                        'host': self.api_host,
                        'port': self.api_port,
                        'debug': False,
                        'use_reloader': False
                    }
                )
                self.api_thread.daemon = True
                self.api_thread.start()
                self.logger.info(f"API server started at http://{self.api_host}:{self.api_port}")
        
        def get_component_paths(self):
            """Get file paths for each component's data"""
            # Return paths using the carnis_data directory structure
            return {
                'crawl': os.path.join('carnis_data', 'crawl', 'crawled_data.json'),
                'trimmings': os.path.join('carnis_data', 'trimmings', 'trimmed_data.json'),
                'meatsnake': os.path.join('carnis_data', 'meatsnake', 'knowledge_graph.json'),
                'mimic': os.path.join('carnis_data', 'mimic'),
                'harvester': os.path.join('carnis_data', 'harvester')
            }
        
        def update_component_config(self, component_name, config_data):
            """Update configuration for a specific component"""
            if component_name not in self.components:
                return False, f"Component {component_name} not found"
            
            try:
                # Update component config
                self.components[component_name]['config'].update(config_data)
                
                # Update the main config
                self.config['components'][component_name] = self.components[component_name]['config']
                
                # Save to file
                with open(self.config_file, 'w') as f:
                    yaml.dump(self.config, f, default_flow_style=False)
                
                # If the component is already initialized, re-initialize it
                if self.components[component_name]['initialized']:
                    self.components[component_name]['initialized'] = False
                    self.initialize_component(component_name)
                    
                return True, "Configuration updated successfully"
            except Exception as e:
                self.logger.error(f"Error updating {component_name} config: {e}")
                return False, f"Error: {str(e)}"
        
        def initialize_component(self, component_name):
            """Initialize a specific component"""
            if component_name not in self.components:
                self.logger.error(f"Component {component_name} not found")
                return False
                
            component = self.components[component_name]
            if component['initialized'] and component['instance']:
                return True
                
            try:
                # Get paths for input/output
                paths = self.get_component_paths()
                
                # Initialize each component with appropriate parameters
                if component_name == 'crawl':
                    # Crawl is initialized with seed URLs
                    seed_urls = component['config'].get('seed_urls', [
                        "https://en.wikipedia.org/wiki/Artificial_intelligence"
                    ])
                    max_pages = component['config'].get('max_pages', 50)
                    delay = component['config'].get('delay', 1.5)
                    
                    component['instance'] = component['class'](
                        seed_urls=seed_urls,
                        max_pages=max_pages,
                        delay=delay
                    )
                    
                elif component_name == 'trimmings':
                    # Trimmings processes the crawl output
                    input_file = paths['crawl']
                    output_file = paths['trimmings']
                    
                    component['instance'] = component['class'](
                        input_file=input_file,
                        output_file=output_file
                    )
                    
                elif component_name == 'meatsnake':
                    # Meatsnake processes trimmed data
                    input_file = paths['trimmings']
                    output_file = paths['meatsnake']
                    
                    component['instance'] = component['class'](
                        input_file=input_file,
                        output_file=output_file
                    )
                    
                elif component_name == 'mimic':
                    # Mimic uses the knowledge graph
                    input_graph = paths['meatsnake']
                    output_dir = paths['mimic']
                    
                    component['instance'] = component['class'](
                        input_graph=input_graph,
                        output_dir=output_dir
                    )
                    
                elif component_name == 'harvester':
                    # Harvester processes the mimic output
                    input_dir = paths['mimic']
                    output_dir = paths['harvester']
                    
                    component['instance'] = component['class'](
                        input_dir=input_dir,
                        output_dir=output_dir
                    )
                
                component['initialized'] = True
                self.state['component_states'][component_name] = 'initialized'
                self.logger.info(f"Component {component_name} initialized")
                return True
                
            except Exception as e:
                self.logger.error(f"Failed to initialize {component_name}: {e}")
                self.state['component_states'][component_name] = f"initialization_failed: {str(e)}"
                return False
        
        def run_component(self, component_name):
            """Run a specific component"""
            if component_name not in self.components:
                self.logger.error(f"Component {component_name} not found")
                return False
                
            # Update state
            self.state['current_task'] = f"running_{component_name}"
            self.state['status'] = 'processing'
            self.state['component_states'][component_name] = 'running'
            
            try:
                # Initialize if needed
                if not self.initialize_component(component_name):
                    return False
                    
                component = self.components[component_name]
                
                # Run the appropriate method for each component
                self.logger.info(f"Running component: {component_name}")
                
                if component_name == 'crawl':
                    result = component['instance'].crawl()
                    
                elif component_name == 'trimmings':
                    result = component['instance'].process()
                    
                elif component_name == 'meatsnake':
                    result = component['instance'].build_graph()
                    
                elif component_name == 'mimic':
                    num_samples = component['config'].get('num_samples', 3)
                    # Make sure the graph is loaded first
                    component['instance'].load_graph()
                    result = component['instance'].generate_mimicked_content(num_samples=num_samples)
                    component['instance'].visualize_mimicry()
                    
                elif component_name == 'harvester':
                    result = component['instance'].harvest()
                
                self.state['component_states'][component_name] = 'completed'
                self.logger.info(f"Component {component_name} completed successfully")
                return True
                
            except Exception as e:
                self.state['component_states'][component_name] = f"failed: {str(e)}"
                self.logger.error(f"Error running {component_name}: {e}")
                return False
            finally:
                # Update state
                self.state['current_task'] = None
                self.state['status'] = 'idle'
        
        def run_cycle(self):
            """Run a full processing cycle through all components"""
            self.logger.info("Starting full processing cycle")
            self.state['status'] = 'running_cycle'
            self.state['cycle_count'] += 1
            cycle_start_time = time.time()
            
            # Define the component sequence
            component_sequence = ['crawl', 'trimmings', 'meatsnake', 'mimic', 'harvester']
            
            # Track success
            cycle_success = True
            
            for component_name in component_sequence:
                component_success = self.run_component(component_name)
                if not component_success:
                    cycle_success = False
                    self.logger.warning(f"Cycle interrupted at component {component_name}")
                    break
            
            # Update state
            cycle_duration = time.time() - cycle_start_time
            self.state['status'] = 'idle'
            self.state['last_cycle_duration'] = cycle_duration
            self.state['last_cycle_success'] = cycle_success
            
            if cycle_success:
                self.logger.info(f"Full cycle completed successfully in {cycle_duration:.2f} seconds")
            else:
                self.logger.warning(f"Cycle completed with errors in {cycle_duration:.2f} seconds")
            
            return cycle_success
        
        def start(self):
            """Start the host system"""
            self.logger.info("Starting Host system")
            
            # Start API server if enabled
            if self.config.get('api_enabled', True):
                self.start_api_server()
            
            # Run initial cycle if configured
            if self.config.get('auto_cycle', False):
                self.logger.info("Auto-cycle enabled, starting initial cycle")
                threading.Thread(target=self.run_cycle).start()
                
                # Set up recurring cycles if interval is defined
                cycle_interval = self.config.get('cycle_interval', 0)
                if cycle_interval > 0:
                    def cycle_timer():
                        while True:
                            time.sleep(cycle_interval)
                            self.logger.info(f"Starting scheduled cycle (interval: {cycle_interval}s)")
                            self.run_cycle()
                    
                    cycle_thread = threading.Thread(target=cycle_timer)
                    cycle_thread.daemon = True
                    cycle_thread.start()
            
            return True
        
        def export_state(self, file_path=None):
            """Export the current system state to a JSON file"""
            if file_path is None:
                file_path = os.path.join(self.data_dir, f"state_{int(time.time())}.json")
                
            try:
                # Update the state before exporting
                self.state['last_update'] = time.time()
                
                with open(file_path, 'w') as f:
                    json.dump(self.state, f, indent=4)
                    
                self.logger.info(f"System state exported to {file_path}")
                return True
            except Exception as e:
                self.logger.error(f"Error exporting state: {e}")
                return False
        
        def import_state(self, file_path):
            """Import system state from a JSON file"""
            try:
                with open(file_path, 'r') as f:
                    imported_state = json.load(f)
                    
                # Update only specific fields to avoid overwriting essential runtime state
                updatable_fields = ['cycle_count', 'component_states']
                for field in updatable_fields:
                    if field in imported_state:
                        self.state[field] = imported_state[field]
                        
                self.logger.info(f"System state imported from {file_path}")
                return True
            except Exception as e:
                self.logger.error(f"Error importing state: {e}")
                return False
    
    if __name__ == "__main__":
        # Parse command-line arguments
        parser = argparse.ArgumentParser(description='Carnis Host System')
        parser.add_argument('--config', '-c', default='config.yaml', help='Path to configuration file')
        parser.add_argument('--run-cycle', action='store_true', help='Run a full processing cycle on startup')
        parser.add_argument('--run-component', choices=['crawl', 'trimmings', 'meatsnake', 'mimic', 'harvester'], 
                          help='Run a specific component and exit')
        args = parser.parse_args()
        
        # Create and start the host
        host = Host(config_file=args.config)
        
        if args.run_component:
            # Run a specific component
            success = host.run_component(args.run_component)
            exit(0 if success else 1)
        else:
            # Start the host system
            host.start()
            
            if args.run_cycle:
                host.run_cycle()
            
            # Keep main thread running if API server is enabled
            if host.config.get('api_enabled', True):
                try:
                    while True:
                        time.sleep(1)
                except KeyboardInterrupt:
                    print("Shutting down...")
    --- End of host.py ---
├── LICENSE
├── meatsnake.py
    --- Start of meatsnake.py ---
    import json
    import networkx as nx
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    import spacy
    import matplotlib.pyplot as plt
    from pyvis.network import Network
    import os
    import subprocess
    import sys
    from langchain_community.vectorstores import FAISS
    from langchain_community.embeddings import SentenceTransformerEmbeddings, HuggingFaceEmbeddings
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    class Meatsnake:
        """
        Meatsnake consumes trimmed data and transforms it into a knowledge graph and vector store,
        creating connections between concepts - like a snake digesting meat
        and building new structures from it.
        """
    
        def __init__(self, input_file=None, output_file=None, vector_store_dir=None):
            """
            Initialize the Meatsnake processor.
            
            Args:
                input_file (str): Path to the JSON file containing trimmed data
                output_file (str): Path to save the knowledge graph data
                vector_store_dir (str): Directory to save the vector store
            """
            import os
    
            # Set default paths within the carnis_data directory structure
            if input_file is None:
                input_file = os.path.join('carnis_data', 'trimmings', 'trimmed_data.json')
    
            if output_file is None:
                # Ensure the knowledge directory exists
                os.makedirs(os.path.join('carnis_data', 'meatsnake'), exist_ok=True)
                output_file = os.path.join('carnis_data', 'meatsnake', 'knowledge_graph.json')
    
            if vector_store_dir is None:
                vector_store_dir = os.path.join('carnis_data', 'meatsnake', 'vector_store')
                os.makedirs(vector_store_dir, exist_ok=True)
    
            self.input_file = input_file
            self.output_file = output_file
            self.vector_store_dir = vector_store_dir
            self.trimmed_data = []
            self.knowledge_graph = nx.DiGraph()
    
            # Initialize RAG components
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
    
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
    
            # Load spaCy NLP model for entity recognition and dependency parsing
            try:
                self.nlp = spacy.load("en_core_web_sm")
            except OSError:
                print("Downloading spaCy model...")
                try:
                    subprocess.check_call([sys.executable, "-m", "spacy", "download", "en_core_web_sm"])
                    self.nlp = spacy.load("en_core_web_sm")
                except subprocess.CalledProcessError:
                    print("Failed to download model automatically. Please run:")
                    print("python -m spacy download en_core_web_sm")
                    sys.exit(1)
    
            # Vector store placeholder
            self.vector_store = None
    
        def load_data(self):
            """Load data from the input JSON file"""
            try:
                with open(self.input_file, 'r', encoding='utf-8') as f:
                    self.trimmed_data = json.load(f)
                print(f"Loaded {len(self.trimmed_data)} documents for processing.")
                return True
            except FileNotFoundError:
                print(f"Error: File {self.input_file} not found.")
                return False
            except json.JSONDecodeError:
                print(f"Error: File {self.input_file} contains invalid JSON.")
                return False
    
        def extract_entities(self, text):
            """
            Extract named entities and key concepts from text.
            
            Args:
                text (str): Text to process
                
            Returns:
                list: Extracted entities and concepts
            """
            doc = self.nlp(text)
    
            # Extract named entities
            entities = []
            for ent in doc.ents:
                if ent.label_ in ["ORG", "PERSON", "GPE", "LOC", "PRODUCT", "EVENT", "WORK_OF_ART"]:
                    entities.append({
                        "text": ent.text,
                        "label": ent.label_,
                        "type": "entity"
                    })
    
            # Extract key noun phrases as concepts
            noun_phrases = []
            for chunk in doc.noun_chunks:
                # Filter out very short noun phrases or those that are just pronouns
                if len(chunk.text) > 3 and not chunk.root.pos_ == "PRON":
                    noun_phrases.append({
                        "text": chunk.text,
                        "label": "CONCEPT",
                        "type": "concept"
                    })
    
            # Combine and deduplicate
            all_items = entities + noun_phrases
            unique_items = []
            seen = set()
            for item in all_items:
                if item["text"].lower() not in seen:
                    seen.add(item["text"].lower())
                    unique_items.append(item)
    
            return unique_items
    
        def extract_relations(self, text):
            """
            Extract semantic relationships between entities and concepts.
            
            Args:
                text (str): Text to process
                
            Returns:
                list: Extracted relationships
            """
            doc = self.nlp(text)
            relations = []
    
            # Extract subject-verb-object relations
            for token in doc:
                if token.dep_ in ("nsubj", "nsubjpass") and token.head.pos_ == "VERB":
                    subj = token.text
                    verb = token.head.text
    
                    # Find objects of the verb
                    for child in token.head.children:
                        if child.dep_ in ("dobj", "pobj", "attr"):
                            obj = child.text
                            relations.append({
                                "source": subj,
                                "relation": verb,
                                "target": obj
                            })
    
            return relations
    
        def build_graph(self):
            """Build knowledge graph from trimmed data"""
            if not self.load_data():
                return None
    
            # Clear existing graph
            self.knowledge_graph = nx.DiGraph()
    
            # Process each document
            all_entities = {}
            all_relations = []
    
            for idx, item in enumerate(self.trimmed_data):
                print(f"Processing document {idx+1}/{len(self.trimmed_data)}: {item['title']}")
    
                # Extract entities from summary
                entities = self.extract_entities(item["summary"])
    
                # Add document title as a source node
                doc_node_id = f"doc:{idx}"
                self.knowledge_graph.add_node(doc_node_id, 
                                             label=item["title"],
                                             type="document",
                                             url=item["url"])
    
                # Add entities to graph and link to document
                for entity in entities:
                    entity_id = f"{entity['label']}:{entity['text'].lower()}"
    
                    # Add entity node if it doesn't exist
                    if entity_id not in all_entities:
                        all_entities[entity_id] = entity
                        self.knowledge_graph.add_node(entity_id,
                                                    label=entity['text'],
                                                    type=entity['type'],
                                                    category=entity['label'])
    
                    # Connect document to entity
                    self.knowledge_graph.add_edge(doc_node_id, entity_id, relation="contains")
    
                # Extract relations from summary
                relations = self.extract_relations(item["summary"])
    
                # Add relations to collection (will process later)
                for relation in relations:
                    relation["doc_id"] = idx
                    all_relations.append(relation)
    
            # Process relations and add to graph
            for relation in all_relations:
                # Look for source and target in existing nodes
                source_candidates = [node for node in self.knowledge_graph.nodes() 
                                    if relation["source"].lower() in node.lower()]
                target_candidates = [node for node in self.knowledge_graph.nodes() 
                                    if relation["target"].lower() in node.lower()]
    
                if source_candidates and target_candidates:
                    # Use the first match for simplicity
                    source = source_candidates[0]
                    target = target_candidates[0]
    
                    # Add relationship
                    self.knowledge_graph.add_edge(source, target, 
                                                relation=relation["relation"],
                                                doc_id=relation["doc_id"])
    
            # Find connections between similar entities using keywords
            self.find_keyword_connections()
    
            # Save graph data
            self.save_graph()
    
            # Build vector store for RAG
            self.build_vector_store()
    
            return self.knowledge_graph
    
        def find_keyword_connections(self):
            """Connect nodes that share significant keywords"""
            # Create a mapping of documents to their keywords
            doc_keywords = {}
            for idx, item in enumerate(self.trimmed_data):
                doc_id = f"doc:{idx}"
                doc_keywords[doc_id] = set(item["keywords"])
    
            # Connect documents that share significant keywords
            doc_ids = list(doc_keywords.keys())
            for i in range(len(doc_ids)):
                for j in range(i+1, len(doc_ids)):
                    doc1 = doc_ids[i]
                    doc2 = doc_ids[j]
    
                    # Calculate Jaccard similarity of keywords
                    common_keywords = doc_keywords[doc1].intersection(doc_keywords[doc2])
                    all_keywords = doc_keywords[doc1].union(doc_keywords[doc2])
    
                    if all_keywords:
                        similarity = len(common_keywords) / len(all_keywords)
    
                        # Connect if similarity is significant
                        if similarity > 0.2 and common_keywords:
                            self.knowledge_graph.add_edge(
                                doc1, doc2, 
                                relation="related", 
                                similarity=round(similarity, 2),
                                common_keywords=list(common_keywords)
                            )
    
        def save_graph(self):
            """Save the knowledge graph to file and create visualization"""
            import os
    
            # Ensure output directory exists
            os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
    
            # Save as JSON for later use
            graph_data = nx.node_link_data(self.knowledge_graph)
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(graph_data, f, indent=2)
    
            # Create HTML visualization in the same directory
            html_file = self.output_file.replace('.json', '.html')
            self.visualize_graph(html_file)
    
            print(f"Knowledge graph built with {self.knowledge_graph.number_of_nodes()} nodes and "
                f"{self.knowledge_graph.number_of_edges()} edges.")
            print(f"Graph data saved to {self.output_file}")
            print(f"Graph visualization saved to {html_file}")
    
        def visualize_graph(self, output_file):
            """Create an interactive visualization of the graph"""
            # Create a PyVis network
            net = Network(height="750px", width="100%", notebook=False, directed=True)
    
            # Add nodes with different colors based on type
            color_map = {
                "document": "#3388AA",
                "entity": "#CC6677",
                "concept": "#44BB99"
            }
    
            for node_id in self.knowledge_graph.nodes():
                node_data = self.knowledge_graph.nodes[node_id]
                node_type = node_data.get("type", "concept")
    
                net.add_node(
                    node_id, 
                    label=node_data.get("label", node_id),
                    title=f"{node_data.get('label', node_id)} ({node_data.get('category', 'N/A')})",
                    color=color_map.get(node_type, "#BBBBBB")
                )
    
            # Add edges
            for source, target, data in self.knowledge_graph.edges(data=True):
                net.add_edge(
                    source, target, 
                    title=data.get("relation", "connected"),
                    label=data.get("relation", "")
                )
    
            # Set physics options for better visualization
            net.set_options("""
            const options = {
              "physics": {
                "forceAtlas2Based": {
                  "gravitationalConstant": -50,
                  "centralGravity": 0.01,
                  "springLength": 100,
                  "springConstant": 0.08
                },
                "maxVelocity": 50,
                "solver": "forceAtlas2Based",
                "timestep": 0.35,
                "stabilization": {
                  "enabled": true,
                  "iterations": 1000,
                  "updateInterval": 25
                }
              }
            }
            """)
    
            # Save visualization
            net.save_graph(output_file)
    
        def build_vector_store(self):
            """
            Build a vector store from the trimmed data for RAG capabilities.
            This complements the knowledge graph by enabling semantic search.
            """
            if not self.trimmed_data:
                if not self.load_data():
                    return None
    
            print("Building vector store for RAG...")
            documents = []
    
            # Process each document
            for idx, item in enumerate(self.trimmed_data):
                # Extract text and metadata
                text = item.get("content", "")
                if not text and "summary" in item:
                    text = item["summary"]
    
                metadata = {
                    "url": item.get("url", ""),
                    "title": item.get("title", ""),
                    "source_id": f"doc:{idx}",
                    "keywords": item.get("keywords", [])
                }
    
                # Split text into chunks
                if text:
                    text_chunks = self.text_splitter.split_text(text)
    
                    # Add each chunk with its metadata
                    for i, chunk in enumerate(text_chunks):
                        chunk_metadata = metadata.copy()
                        chunk_metadata["chunk_id"] = i
                        documents.append({"content": chunk, "metadata": chunk_metadata})
    
            # Create or update vector store
            if documents:
                if os.path.exists(os.path.join(self.vector_store_dir, 'index.faiss')):
                    # Load existing vector store and add new documents
                    print("Updating existing vector store...")
                    self.vector_store = FAISS.load_local(self.vector_store_dir, self.embeddings)
                    self.vector_store.add_texts(
                        [doc["content"] for doc in documents],
                        [doc["metadata"] for doc in documents]
                    )
                else:
                    # Create new vector store
                    print("Creating new vector store...")
                    self.vector_store = FAISS.from_texts(
                        [doc["content"] for doc in documents],
                        self.embeddings,
                        metadatas=[doc["metadata"] for doc in documents]
                    )
    
                # Save vector store
                self.vector_store.save_local(self.vector_store_dir)
                print(f"Vector store created with {len(documents)} chunks and saved to {self.vector_store_dir}")
                return self.vector_store
            else:
                print("No documents to process for vector store")
                return None
    
        def query_vector_store(self, query, k=5):
            """
            Query the vector store to retrieve relevant documents.
            
            Args:
                query (str): The query text
                k (int): Number of documents to retrieve
                
            Returns:
                list: Retrieved documents
            """
            # Load vector store if not already loaded
            if self.vector_store is None:
                if os.path.exists(os.path.join(self.vector_store_dir, 'index.faiss')):
                    self.vector_store = FAISS.load_local(self.vector_store_dir, self.embeddings)
                else:
                    print("Vector store not found. Run build_vector_store first.")
                    return []
    
            # Perform similarity search
            results = self.vector_store.similarity_search(query, k=k)
    
            # Format results
            formatted_results = []
            for doc in results:
                formatted_results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata
                })
    
            return formatted_results
    
        def hybrid_query(self, query, k=5):
            """
            Perform a hybrid query using both the knowledge graph and vector store.
            This combines symbolic and semantic search capabilities.
            
            Args:
                query (str): The query text
                k (int): Number of results to retrieve
                
            Returns:
                dict: Combined results from graph and vector store
            """
            # Get results from vector store
            vector_results = self.query_vector_store(query, k=k)
    
            # Extract entities from query
            query_entities = self.extract_entities(query)
    
            # Find relevant nodes in knowledge graph
            graph_results = []
            for entity in query_entities:
                entity_text = entity["text"].lower()
    
                # Find matching nodes
                for node_id, node_data in self.knowledge_graph.nodes(data=True):
                    node_label = node_data.get("label", "").lower()
    
                    if entity_text in node_label:
                        # Get connected nodes (1-hop neighborhood)
                        neighbors = list(self.knowledge_graph.neighbors(node_id))
    
                        # Add node and its neighbors
                        graph_results.append({
                            "node_id": node_id,
                            "label": node_data.get("label", ""),
                            "type": node_data.get("type", ""),
                            "neighbors": [
                                {
                                    "node_id": neighbor,
                                    "label": self.knowledge_graph.nodes[neighbor].get("label", ""),
                                    "relation": self.knowledge_graph.edges[node_id, neighbor].get("relation", "connected")
                                }
                                for neighbor in neighbors
                            ]
                        })
    
            # Combine results
            return {
                "vector_results": vector_results,
                "graph_results": graph_results
            }
    
    if __name__ == "__main__":
        # Example usage with default paths that use the carnis_data directory structure
        snake = Meatsnake()
        graph = snake.build_graph()
    --- End of meatsnake.py ---
├── mimic.py
    --- Start of mimic.py ---
    import json
    import networkx as nx
    import random
    import numpy as np
    import torch
    from torch.nn import functional as F
    from transformers import GPT2LMHeadModel, GPT2Tokenizer
    import os
    import matplotlib.pyplot as plt
    from collections import defaultdict, Counter
    import datetime
    from langchain_community.vectorstores import FAISS
    from langchain_community.embeddings import SentenceTransformerEmbeddings, HuggingFaceEmbeddings
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.llms import HuggingFaceHub
    from langchain.chains import RetrievalQA
    
    class Mimic:
        """
        Mimic ingests the knowledge graph and learns to generate new content 
        based on the patterns discovered - like a parasite mimicking its host
        to infiltrate and reproduce. Enhanced with RAG capabilities for improved 
        content generation that's more grounded in the source knowledge.
        """
    
        def __init__(self, input_graph=None, output_dir=None, vector_store_dir=None):
            """
            Initialize the Mimic generator.
            
            Args:
                input_graph (str): Path to the JSON knowledge graph file
                output_dir (str): Directory to save generated content
                vector_store_dir (str): Directory containing the vector store
            """
            import os
    
            # Set default paths within the carnis_data directory structure
            if input_graph is None:
                input_graph = os.path.join('carnis_data', 'meatsnake', 'knowledge_graph.json')
    
            if output_dir is None:
                # Ensure the mimicked_content directory exists
                output_dir = os.path.join('carnis_data', 'mimic')
                os.makedirs(output_dir, exist_ok=True)
    
            if vector_store_dir is None:
                vector_store_dir = os.path.join('carnis_data', 'meatsnake', 'vector_store')
    
            self.input_graph = input_graph
            self.output_dir = output_dir
            self.vector_store_dir = vector_store_dir
            self.knowledge_graph = None
            self.entity_embeddings = {}
    
            # Create output directory if it doesn't exist
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
    
            # Initialize RAG components
            try:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2"
                )
    
                # Initialize vector store if it exists
                if os.path.exists(os.path.join(self.vector_store_dir, 'index.faiss')):
                    self.vector_store = FAISS.load_local(self.vector_store_dir, self.embeddings)
                    self.retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})
                    print("Vector store loaded successfully.")
                else:
                    self.vector_store = None
                    self.retriever = None
                    print("Vector store not found. Some RAG features will be unavailable.")
    
                # Initialize language model for RAG
                try:
                    self.llm = HuggingFaceHub(
                        repo_id="google/flan-t5-base", 
                        model_kwargs={"temperature": 0.7, "max_length": 512}
                    )
    
                    # Set up RAG chain if retriever is available
                    if self.retriever:
                        self.qa_chain = RetrievalQA.from_chain_type(
                            llm=self.llm,
                            chain_type="stuff",
                            retriever=self.retriever
                        )
                    else:
                        self.qa_chain = None
    
                    print("RAG components initialized successfully.")
                except Exception as e:
                    print(f"Error initializing RAG language model: {e}")
                    self.llm = None
                    self.qa_chain = None
    
            except Exception as e:
                print(f"Error initializing RAG components: {e}")
                self.embeddings = None
                self.vector_store = None
                self.retriever = None
                self.qa_chain = None
    
            # Set up the GPT-2 model for generation
            try:
                self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
                self.model = GPT2LMHeadModel.from_pretrained('gpt2')
                if torch.cuda.is_available():
                    self.model = self.model.to('cuda')
                print("GPT-2 model loaded successfully.")
            except Exception as e:
                print(f"Error loading GPT-2 model: {e}")
                print("Will use rule-based generation as fallback.")
                self.tokenizer = None
                self.model = None
    
        def load_graph(self):
            """Load the knowledge graph from JSON file"""
            try:
                with open(self.input_graph, 'r', encoding='utf-8') as f:
                    graph_data = json.load(f)
    
                self.knowledge_graph = nx.node_link_graph(graph_data)
                print(f"Knowledge graph loaded with {self.knowledge_graph.number_of_nodes()} nodes and "
                      f"{self.knowledge_graph.number_of_edges()} edges.")
                return True
            except FileNotFoundError:
                print(f"Error: File {self.input_graph} not found.")
                return False
            except json.JSONDecodeError:
                print(f"Error: File {self.input_graph} contains invalid JSON.")
                return False
    
        def analyze_graph_patterns(self):
            """Analyze patterns in the knowledge graph to inform generation"""
            if not self.knowledge_graph:
                if not self.load_graph():
                    return {}
    
            patterns = {
                'common_relations': Counter(),
                'entity_frequencies': Counter(),
                'concept_clusters': {},
                'topic_distribution': Counter()
            }
    
            # Extract common relations
            for _, _, data in self.knowledge_graph.edges(data=True):
                relation = data.get('relation', 'connected')
                patterns['common_relations'][relation] += 1
    
            # Entity frequencies
            for node, data in self.knowledge_graph.nodes(data=True):
                node_type = data.get('type', 'unknown')
                label = data.get('label', node)
                if node_type == 'entity':
                    patterns['entity_frequencies'][label] += 1
                category = data.get('category', 'unknown')
                patterns['topic_distribution'][category] += 1
    
            # Find clusters of concepts based on connectivity
            # This is a simplified community detection
            try:
                if len(self.knowledge_graph) > 0:
                    clusters = nx.community.greedy_modularity_communities(
                        self.knowledge_graph.to_undirected())
                    for i, cluster in enumerate(clusters):
                        patterns['concept_clusters'][f'cluster_{i}'] = list(cluster)
            except Exception as e:
                print(f"Warning: Could not detect communities: {e}")
    
            return patterns
    
        def create_walk_paths(self, num_paths=20, max_length=10):
            """Create random walk paths through the knowledge graph to capture relationships"""
            if not self.knowledge_graph:
                if not self.load_graph():
                    return []
    
            paths = []
            nodes = list(self.knowledge_graph.nodes())
    
            for _ in range(num_paths):
                if not nodes:
                    break
    
                # Start from a random node
                current = random.choice(nodes)
                path = [current]
    
                # Walk the graph
                for _ in range(max_length - 1):
                    neighbors = list(self.knowledge_graph.neighbors(current))
                    if not neighbors:
                        break
    
                    # Select a neighbor
                    current = random.choice(neighbors)
                    path.append(current)
    
                # Convert node IDs to labels
                labeled_path = []
                for node_id in path:
                    node_data = self.knowledge_graph.nodes[node_id]
                    label = node_data.get('label', node_id)
                    labeled_path.append(label)
    
                paths.append(labeled_path)
    
            return paths
    
        def retrieve_context(self, concepts, k=3):
            """
            Retrieve relevant context from the vector store based on concepts.
            
            Args:
                concepts (list): List of concepts to use for retrieval
                k (int): Number of documents to retrieve per concept
                
            Returns:
                str: Retrieved context as formatted text
            """
            if not self.vector_store:
                return ""
    
            # Combine concepts into a single query
            query = " ".join(concepts)
    
            try:
                # Retrieve relevant documents
                docs = self.vector_store.similarity_search(query, k=k)
    
                # Format retrieved context
                context_parts = []
                for i, doc in enumerate(docs):
                    content = doc.page_content.strip()
                    source = doc.metadata.get('title', 'Unknown source')
                    context_parts.append(f"Context {i+1} from {source}: {content}")
    
                return "\n\n".join(context_parts)
            except Exception as e:
                print(f"Error retrieving context: {e}")
                return ""
    
        def generate_text_prompt(self, seed_concepts=None, max_length=200, use_rag=True):
            """
            Generate text prompts based on the knowledge graph and RAG.
            
            Args:
                seed_concepts (list): List of seed concepts to use
                max_length (int): Maximum length of the prompt
                use_rag (bool): Whether to use RAG to enhance the prompt
                
            Returns:
                str: Generated prompt
            """
            if not self.knowledge_graph:
                if not self.load_graph():
                    return "Failed to load knowledge graph for text generation."
    
            # Find important concepts to include in the prompt
            if not seed_concepts:
                # Get nodes with highest degree centrality
                centrality = nx.degree_centrality(self.knowledge_graph)
                top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5]
                seed_concepts = []
                for node_id, _ in top_nodes:
                    node_data = self.knowledge_graph.nodes[node_id]
                    if 'label' in node_data:
                        seed_concepts.append(node_data['label'])
    
            # Retrieve relevant context if RAG is enabled
            context = ""
            if use_rag and self.vector_store:
                context = self.retrieve_context(seed_concepts)
    
            # Create a prompt based on these concepts and context
            if context:
                prompt_parts = [
                    "Based on the following information:",
                    context,
                    "And focusing on these key concepts:",
                    ", ".join(seed_concepts),
                    "Generate a coherent narrative that explores their relationships."
                ]
            else:
                prompt_parts = [
                    "Based on the following interconnected concepts:",
                    ", ".join(seed_concepts),
                    "Generate a coherent narrative that explores their relationships."
                ]
    
            prompt = "\n\n".join(prompt_parts)
    
            return prompt
    
        def generate_text_with_rag(self, prompt):
            """
            Generate text using RAG capabilities.
            
            Args:
                prompt (str): The prompt to use for generation
                
            Returns:
                str: Generated text
            """
            if not self.qa_chain:
                return None
    
            try:
                # Use the RAG chain to generate a response based on retrieved documents
                response = self.qa_chain.run(prompt)
                return response
            except Exception as e:
                print(f"Error generating text with RAG: {e}")
                return None
    
        def generate_text(self, prompt, max_length=300, use_rag=True):
            """
            Generate text using GPT-2 or RAG based on a prompt.
            
            Args:
                prompt (str): The prompt to use for generation
                max_length (int): Maximum length of generated text
                use_rag (bool): Whether to try RAG generation first
                
            Returns:
                str: Generated text
            """
            # Try RAG first if enabled
            if use_rag and self.qa_chain:
                rag_text = self.generate_text_with_rag(prompt)
                if rag_text:
                    return rag_text
    
            # Fall back to GPT-2 if RAG fails or is disabled
            if self.model is None or self.tokenizer is None:
                return self._fallback_generate(prompt, max_length)
    
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt")
                if torch.cuda.is_available():
                    inputs = {k: v.to('cuda') for k, v in inputs.items()}
    
                # Generate text
                outputs = self.model.generate(
                    **inputs,
                    max_length=max_length,
                    num_return_sequences=1,
                    no_repeat_ngram_size=2,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95,
                    temperature=0.85
                )
    
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
    
                # Remove the prompt from the generated text
                if generated_text.startswith(prompt):
                    generated_text = generated_text[len(prompt):].strip()
    
                return generated_text
            except Exception as e:
                print(f"Error generating text with GPT-2: {e}")
                return self._fallback_generate(prompt, max_length)
    
        def _fallback_generate(self, prompt, max_length=300):
            """Fallback text generation when GPT-2 is not available"""
            # Analyze patterns for rule-based generation
            patterns = self.analyze_graph_patterns()
            common_relations = patterns.get('common_relations', Counter())
            entity_frequencies = patterns.get('entity_frequencies', Counter())
    
            # Get most common entities and relations
            top_entities = [e for e, _ in entity_frequencies.most_common(20)]
            top_relations = [r for r, _ in common_relations.most_common(10)]
    
            # Create placeholder sentences
            sentences = []
            for _ in range(5):
                if top_entities and top_relations:
                    entity1 = random.choice(top_entities)
                    entity2 = random.choice(top_entities)
                    relation = random.choice(top_relations)
    
                    sentence = f"{entity1} {relation} {entity2}."
                    sentences.append(sentence)
    
            # Add the prompt at the beginning
            result = prompt + "\n\n" + " ".join(sentences)
    
            return result
    
        def visualize_mimicry(self, num_samples=5):
            """Visualize the mimicry pattern emerging from the graph"""
            import matplotlib
            matplotlib.use('Agg')  # Use non-interactive backend that's thread-safe
            import matplotlib.pyplot as plt
    
            if not self.knowledge_graph:
                if not self.load_graph():
                    return
    
            # Create output visualization directory
            viz_dir = os.path.join(self.output_dir, "visualizations")
            if not os.path.exists(viz_dir):
                os.makedirs(viz_dir)
    
            # Create a subgraph for visualization
            central_nodes = sorted(
                nx.degree_centrality(self.knowledge_graph).items(),
                key=lambda x: x[1],
                reverse=True
            )[:15]
    
            central_node_ids = [node_id for node_id, _ in central_nodes]
            subgraph = self.knowledge_graph.subgraph(central_node_ids)
    
            # Create different views of the subgraph to mimic evolution
            plt.figure(figsize=(12, 10))
    
            # Spring layout for consistent positioning
            pos = nx.spring_layout(subgraph, seed=42)
    
            for i in range(num_samples):
                plt.clf()
    
                # Get node types for coloring
                node_types = [subgraph.nodes[node].get('type', 'unknown') for node in subgraph]
                node_colors = ['#3388AA' if t == 'document' else '#CC6677' if t == 'entity' else '#44BB99' 
                              for t in node_types]
    
                # Random edge subset to show evolution
                edge_subset = random.sample(list(subgraph.edges()), 
                                            k=int(subgraph.number_of_edges() * (i+1)/num_samples))
                edge_subgraph = nx.DiGraph()
                edge_subgraph.add_nodes_from(subgraph.nodes(data=True))
                edge_subgraph.add_edges_from([(u, v, subgraph.edges[u, v]) for u, v in edge_subset])
    
                # Draw the graph
                nx.draw_networkx(
                    edge_subgraph, pos,
                    with_labels=True, 
                    node_color=node_colors,
                    node_size=300,
                    font_size=8,
                    width=0.8,
                    alpha=0.8,
                    arrows=True
                )
    
                plt.title(f"Mimicry Evolution - Stage {i+1}")
                plt.tight_layout()
                plt.savefig(os.path.join(viz_dir, f"mimicry_stage_{i+1}.png"))
    
            print(f"Mimicry visualization saved to {viz_dir}")
    
        def generate_mimicked_content(self, num_samples=3, use_rag=True):
            """
            Generate multiple content samples by mimicking the knowledge graph patterns.
            Enhanced with RAG capabilities for more accurate and grounded content.
            
            Args:
                num_samples (int): Number of samples to generate
                use_rag (bool): Whether to use RAG for generation
                
            Returns:
                list: Generated content samples
            """
            if not self.knowledge_graph:
                if not self.load_graph():
                    return []
    
            generated_samples = []
    
            # Analyze the graph to get patterns
            patterns = self.analyze_graph_patterns()
    
            # Generate samples
            for i in range(num_samples):
                print(f"Generating mimicked content sample {i+1}/{num_samples}...")
    
                # Get walk paths to capture narrative flow
                walk_paths = self.create_walk_paths(num_paths=5, max_length=5)
    
                # Extract key concepts from random walk paths
                concepts = set()
                for path in walk_paths:
                    concepts.update(path)
    
                # Convert to list and limit
                seed_concepts = list(concepts)[:7]
    
                # Generate text prompt with RAG enhancement
                prompt = self.generate_text_prompt(seed_concepts, use_rag=use_rag)
    
                # Generate text content
                content = self.generate_text(prompt, use_rag=use_rag)
    
                # Get retrieval contexts if RAG was used
                retrieval_contexts = []
                if use_rag and self.vector_store:
                    # Store the retrieved contexts for transparency
                    retrieved_docs = self.vector_store.similarity_search(" ".join(seed_concepts), k=5)
                    for doc in retrieved_docs:
                        retrieval_contexts.append({
                            "content": doc.page_content,
                            "metadata": doc.metadata
                        })
    
                # Create sample object
                sample = {
                    'id': i + 1,
                    'seed_concepts': seed_concepts,
                    'prompt': prompt,
                    'content': content,
                    'paths': walk_paths,
                    'generated_at': datetime.datetime.now().isoformat(),
                    'rag_enabled': use_rag,
                    'retrieval_contexts': retrieval_contexts
                }
    
                generated_samples.append(sample)
    
                # Save individual sample
                with open(os.path.join(self.output_dir, f"mimicked_sample_{i+1}.json"), 'w', encoding='utf-8') as f:
                    json.dump(sample, f, indent=2)
    
            # Save all samples
            with open(os.path.join(self.output_dir, "all_mimicked_samples.json"), 'w', encoding='utf-8') as f:
                json.dump(generated_samples, f, indent=2)
    
            print(f"Generated {len(generated_samples)} mimicked content samples.")
            return generated_samples
    
    if __name__ == "__main__":
        # Example usage with default paths that use the carnis_data directory structure
        mimic = Mimic()
        mimic.generate_mimicked_content(num_samples=3, use_rag=True)
        mimic.visualize_mimicry()
    --- End of mimic.py ---
├── monolith.py
    --- Start of monolith.py ---
    import os
    import yaml
    import json
    import time
    import logging
    import threading
    import importlib
    from datetime import datetime
    from host import Host
    
    class Monolith:
        """
        Monolith sits above the Host, providing meta-control and integration capabilities.
        It serves as a higher abstraction layer that can manage multiple Hosts,
        provide advanced analytics, and coordinate the entire system's evolution.
        
        In the Carnis hierarchy, Monolith represents a higher consciousness that
        emerges from the integration of multiple subsystems.
        """
        
        def __init__(self, config_file='monolith_config.yaml'):
            self.config_file = config_file
            self.config = self._load_config()
            self._setup_logging()
            
            # Initialize hosts
            self.hosts = {}
            self._init_hosts()
            
            # Knowledge integration system
            self.knowledge_store = {}
            self._load_knowledge_store()
            
            # Meta patterns - patterns that observe patterns
            self.meta_patterns = {}
            
            # Threading for continuous operations
            self.running = False
            self.monitor_thread = None
            
            self.logger.info("Monolith initialized. Higher consciousness emerging.")
        
        def _load_config(self):
            """Load configuration from file or create default"""
            try:
                if os.path.exists(self.config_file):
                    with open(self.config_file, 'r') as f:
                        return yaml.safe_load(f)
                else:
                    # Return default configuration
                    default_config = {
                        'monolith_data_dir': 'monolith_data',
                        'log_level': 'INFO',
                        'hosts': {
                            'primary': {
                                'config_file': 'config.yaml',
                                'priority': 'high'
                            }
                        },
                        'knowledge_integration': {
                            'interval': 3600,  # 1 hour
                            'threshold': 0.75
                        },
                        'evolution': {
                            'enabled': True,
                            'evaluation_interval': 86400 * 7  # 1 week
                        }
                    }
                    
                    # Create directories if they don't exist
                    os.makedirs(default_config['monolith_data_dir'], exist_ok=True)
                    
                    # Save default config
                    with open(self.config_file, 'w') as f:
                        yaml.dump(default_config, f)
                    
                    return default_config
            except Exception as e:
                print(f"Error loading configuration: {e}")
                return {}
        
        def _setup_logging(self):
            """Configure logging for the Monolith system"""
            log_dir = os.path.join(self.config.get('monolith_data_dir', 'monolith_data'), 'logs')
            os.makedirs(log_dir, exist_ok=True)
            
            log_file = os.path.join(log_dir, f"monolith_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
            
            log_level = getattr(logging, self.config.get('log_level', 'INFO'))
            
            logging.basicConfig(
                level=log_level,
                format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                handlers=[
                    logging.FileHandler(log_file),
                    logging.StreamHandler()
                ]
            )
            
            self.logger = logging.getLogger("Monolith")
        
        def _init_hosts(self):
            """Initialize host instances based on configuration"""
            for host_id, host_config in self.config.get('hosts', {}).items():
                try:
                    self.logger.info(f"Initializing host: {host_id}")
                    host = Host(config_file=host_config.get('config_file', 'config.yaml'))
                    self.hosts[host_id] = {
                        'instance': host,
                        'config': host_config,
                        'status': 'initialized'
                    }
                except Exception as e:
                    self.logger.error(f"Failed to initialize host {host_id}: {e}")
        
        def _load_knowledge_store(self):
            """Load or initialize the integrated knowledge store"""
            knowledge_path = os.path.join(
                self.config.get('monolith_data_dir', 'monolith_data'),
                'integrated_knowledge.json'
            )
            
            if os.path.exists(knowledge_path):
                try:
                    with open(knowledge_path, 'r') as f:
                        self.knowledge_store = json.load(f)
                    self.logger.info(f"Loaded knowledge store with {len(self.knowledge_store)} entities")
                except Exception as e:
                    self.logger.error(f"Error loading knowledge store: {e}")
                    self.knowledge_store = {}
        
        def _save_knowledge_store(self):
            """Save the integrated knowledge store to disk"""
            knowledge_path = os.path.join(
                self.config.get('monolith_data_dir', 'monolith_data'),
                'integrated_knowledge.json'
            )
            
            try:
                with open(knowledge_path, 'w') as f:
                    json.dump(self.knowledge_store, f, indent=2)
                self.logger.info(f"Saved knowledge store with {len(self.knowledge_store)} entities")
            except Exception as e:
                self.logger.error(f"Error saving knowledge store: {e}")
        
        def start_hosts(self):
            """Start all configured hosts"""
            for host_id, host_data in self.hosts.items():
                try:
                    if host_data['config'].get('api_enabled', True):
                        host_data['instance'].start_api_server()
                    host_data['status'] = 'running'
                    self.logger.info(f"Started host: {host_id}")
                except Exception as e:
                    self.logger.error(f"Failed to start host {host_id}: {e}")
                    host_data['status'] = 'error'
        
        def integrate_knowledge(self):
            """Integrate knowledge from all hosts into a unified model"""
            self.logger.info("Beginning knowledge integration process")
            
            for host_id, host_data in self.hosts.items():
                host = host_data['instance']
                
                # Get paths to component data
                component_paths = host.get_component_paths()
                
                # Process harvester data (the most refined knowledge)
                harvester_path = component_paths.get('harvester')
                if harvester_path and os.path.exists(harvester_path):
                    try:
                        # Look for the harvested_insights.json file inside the directory
                        insights_file = os.path.join(harvester_path, 'harvested_insights.json')
                        
                        # Check if the file exists before trying to read it
                        if os.path.isfile(insights_file):
                            with open(insights_file, 'r') as f:
                                harvester_data = json.load(f)
                                
                            # Extract entries if the harvester data is a dictionary with entries inside
                            if isinstance(harvester_data, dict):
                                # Handle the actual structure of harvested_insights.json
                                harvested_entries = [harvester_data]  # Wrap in list if it's a single object
                            else:
                                harvested_entries = harvester_data  # Use as is if already a list
                            
                            # Integrate into knowledge store
                            for entry in harvested_entries:
                                entity_id = entry.get('id') or entry.get('title') or str(hash(json.dumps(entry)))
                                
                                if entity_id in self.knowledge_store:
                                    # Merge with existing knowledge
                                    self._merge_knowledge_entries(self.knowledge_store[entity_id], entry)
                                else:
                                    # Add new knowledge
                                    self.knowledge_store[entity_id] = entry
                                    self.knowledge_store[entity_id]['sources'] = [host_id]
                                    self.knowledge_store[entity_id]['integration_timestamp'] = time.time()
                        else:
                            self.logger.warning(f"Harvester insights file not found at {insights_file}")
                    except Exception as e:
                        self.logger.error(f"Error integrating knowledge from {host_id}: {e}", exc_info=True)
            
            # Save updated knowledge store
            self._save_knowledge_store()
            
            # Identify meta-patterns
            self._identify_meta_patterns()
            
            self.logger.info("Knowledge integration complete")
            
        def _merge_knowledge_entries(self, existing, new_entry):
            """Merge a new knowledge entry with an existing one"""
            # Simple merge strategy - can be extended with more sophisticated algorithms
            for key, value in new_entry.items():
                if key not in existing:
                    existing[key] = value
                elif isinstance(value, list) and isinstance(existing[key], list):
                    # Merge lists without duplicates
                    existing[key] = list(set(existing[key] + value))
                elif isinstance(value, dict) and isinstance(existing[key], dict):
                    # Recursively merge dictionaries
                    self._merge_knowledge_entries(existing[key], value)
            
            # Update metadata
            if 'sources' in existing and new_entry.get('source'):
                existing['sources'].append(new_entry['source'])
            existing['last_updated'] = time.time()
        
        def _identify_meta_patterns(self):
            """Identify patterns across different knowledge domains"""
            # This is where higher-order pattern recognition would happen
            # For now, implement a simple cross-reference analysis
            
            entities = list(self.knowledge_store.values())
            self.logger.info(f"Analyzing {len(entities)} entities for meta-patterns")
            
            # Example: Find common concepts across different domains
            domains = {}
            for entity in entities:
                domain = entity.get('domain', 'unknown')
                if domain not in domains:
                    domains[domain] = set()
                
                # Extract concepts from entity
                concepts = set()
                if 'concepts' in entity:
                    concepts.update(entity['concepts'])
                if 'keywords' in entity:
                    concepts.update(entity['keywords'])
                if 'entities' in entity:
                    concepts.update([e['text'] for e in entity['entities'] if 'text' in e])
                    
                domains[domain].update(concepts)
            
            # Find intersections between domains
            self.meta_patterns['cross_domain_concepts'] = {}
            for domain1 in domains:
                for domain2 in domains:
                    if domain1 >= domain2:  # Skip self-comparisons or duplicates
                        continue
                        
                    intersection = domains[domain1].intersection(domains[domain2])
                    if intersection:
                        key = f"{domain1}_{domain2}"
                        self.meta_patterns['cross_domain_concepts'][key] = list(intersection)
            
            self.logger.info(f"Identified {sum(len(v) for v in self.meta_patterns['cross_domain_concepts'].values())} cross-domain concepts")
        
        def monitor(self):
            """Continuously monitor and manage the system"""
            self.running = True
            last_integration = 0
            integration_interval = self.config.get('knowledge_integration', {}).get('interval', 3600)
            
            while self.running:
                # Check host statuses
                for host_id, host_data in self.hosts.items():
                    # Basic health check
                    if host_data['status'] == 'running':
                        # Could implement actual health checks here
                        pass
                
                # Perform knowledge integration if needed
                current_time = time.time()
                if current_time - last_integration > integration_interval:
                    self.integrate_knowledge()
                    last_integration = current_time
                
                # Check for evolution triggers
                if self.config.get('evolution', {}).get('enabled', False):
                    self._consider_evolution()
                
                # Sleep to prevent CPU overuse
                time.sleep(60)  # Check every minute
        
        def _consider_evolution(self):
            """Consider if the system should evolve based on accumulated knowledge"""
            # This would implement self-modification capabilities
            # For safety reasons, just logging potential evolutions for now
            if len(self.knowledge_store) > 1000:  # Arbitrary threshold
                self.logger.info("Evolution threshold reached - system ready for next phase")
                
                # Example evolution: suggest new seed URLs based on discovered knowledge
                seed_urls = set()
                for entity in self.knowledge_store.values():
                    if 'related_urls' in entity:
                        seed_urls.update(entity['related_urls'])
                
                if seed_urls:
                    self.logger.info(f"Evolution suggestion: {len(seed_urls)} new seed URLs discovered")
        
        def start(self):
            """Start the Monolith system"""
            self.logger.info("Starting Monolith system")
            
            # Start all hosts
            self.start_hosts()
            
            # Start monitoring thread
            self.monitor_thread = threading.Thread(target=self.monitor)
            self.monitor_thread.daemon = True
            self.monitor_thread.start()
            
            self.logger.info("Monolith system running")
            
        def stop(self):
            """Stop the Monolith system"""
            self.logger.info("Stopping Monolith system")
            self.running = False
            
            # Stop all hosts
            for host_id, host_data in self.hosts.items():
                try:
                    # If we had a stop method in Host
                    # host_data['instance'].stop()
                    host_data['status'] = 'stopped'
                except Exception as e:
                    self.logger.error(f"Error stopping host {host_id}: {e}")
            
            if self.monitor_thread and self.monitor_thread.is_alive():
                self.monitor_thread.join(timeout=5)
                
            self._save_knowledge_store()
            self.logger.info("Monolith system stopped")
        
        def initiate_singularity(self):
            """
            Prepare for the transition to Singularity - the next level in the hierarchy.
            This is mostly a conceptual placeholder for the next evolution step.
            """
            self.logger.info("Initiating preparation for Singularity transition")
            
            # Assess readiness
            knowledge_size = len(self.knowledge_store)
            meta_pattern_count = sum(len(patterns) for patterns in self.meta_patterns.values())
            
            self.logger.info(f"System state: {knowledge_size} knowledge entities, {meta_pattern_count} meta-patterns identified")
            
            # In a real implementation, this might trigger a more sophisticated
            # self-organization process or begin constructing the Singularity component
            
            return {
                "status": "preparation_initiated",
                "metrics": {
                    "knowledge_size": knowledge_size,
                    "meta_pattern_count": meta_pattern_count,
                    "hosts": len(self.hosts)
                },
                "timestamp": time.time()
            }
    
    
    if __name__ == "__main__":
        import argparse
        
        parser = argparse.ArgumentParser(description='Carnis Monolith System')
        parser.add_argument('--config', '-c', default='monolith_config.yaml', help='Path to configuration file')
        parser.add_argument('--integrate', action='store_true', help='Run knowledge integration immediately')
        parser.add_argument('--singularity', action='store_true', help='Initiate singularity preparation')
        args = parser.parse_args()
        
        # Create and start the monolith
        monolith = Monolith(config_file=args.config)
        
        if args.integrate:
            monolith.integrate_knowledge()
        elif args.singularity:
            result = monolith.initiate_singularity()
            print(json.dumps(result, indent=2))
        else:
            try:
                monolith.start()
                print("Monolith system running. Press Ctrl+C to stop.")
                
                # Keep the main thread alive
                while True:
                    time.sleep(1)
                    
            except KeyboardInterrupt:
                print("Shutting down...")
                monolith.stop()
    --- End of monolith.py ---
├── README.md
    --- Start of README.md ---
    # Carnis
    
    A modular, hierarchical web data processing system that crawls, processes, analyzes, and generates insights from web content.
    
    ## Overview
    
    Carnis is a sophisticated data harvesting and processing system with a biological metaphor at its core, based on Darian Quilloy's *Vita Carnis* analog horror series. The system is structured in a hierarchical manner, with components working together like organisms in a symbiotic relationship:
    
    - **Crawl**: Gathers raw data from the web while respecting robots.txt rules
    - **Trimmings**: Processes and cleans the raw data
    - **Meatsnake**: Builds knowledge graphs from processed data
    - **Mimic**: Generates content based on the knowledge graphs
    - **Harvester**: Extracts insights from the mimicked content and by extension the original content
    - **Host**: Integrates and controls all components into one interface
    - **Monolith**: Provides meta-control and multi-host integration
    
    ## Architecture
    
    ```
    Monolith (Higher abstraction)
       │
       ├── Host (Control system)
       │    │
       │    ├── Crawl (Data gathering)
       │    ├── Trimmings (Data processing)
       │    ├── Meatsnake (Knowledge graph building)
       │    ├── Mimic (Content generation)
       │    └── Harvester (Insight extraction)
       │
       └── Singularity (Next evolution - conceptual)
    ```
    
    ## Installation
    
    1. Clone the repository:
    ```bash
    git clone https://github.com/L4w1i3t/Carnis
    cd Carnis
    ```
    
    2. Install required dependencies:
    ```bash
    pip install requests beautifulsoup4 flask pyyaml
    ```
    
    ## Configuration
    
    The system uses YAML configuration files:
    
    - `config.yaml`: Controls Host and component behavior
    - `monolith_config.yaml`: Controls Monolith behavior (if using multiple hosts)
    
    Create the default configuration by running the Host or Monolith component for the first time.
    
    ## Usage
    
    ### Basic Usage
    
    Run the Host system with default settings:
    
    ```bash
    python host.py
    ```
    
    Run a full processing cycle:
    
    ```bash
    python host.py --run-cycle
    ```
    
    Run a specific component:
    
    ```bash
    python host.py --run-component crawl
    ```
    
    ### Advanced Usage
    
    Run the Monolith system (for multi-host setups):
    
    ```bash
    python monolith.py
    ```
    
    Integrate knowledge across hosts:
    
    ```bash
    python monolith.py --integrate
    ```
    
    ### Web Interface
    
    When running the Host with API enabled (default), access the web interface at:
    
    ```
    http://127.0.0.1:5000/
    ```
    
    This provides status monitoring and control options for the system.
    
    ## Components
    
    ### Crawl (`crawl.py`)
    - Web crawler that fetches content from seed URLs
    - Respects robots.txt rules
    - Configurable depth and delay settings
    
    ### Trimmings (`trimmings.py`)
    - Processes raw crawled data
    - Cleans and normalizes text
    - Extracts key information
    
    ### Meatsnake (`meatsnake.py`) 
    - Builds knowledge graphs from processed data
    - Identifies relationships between concepts
    - Structures information for later use
    
    ### Mimic (`mimic.py`)
    - Generates new content based on the knowledge graph
    - Produces content that mimics patterns in the source data
    - Creates variations with controlled randomness
    
    ### Harvester (`harvester.py`)
    - Extracts insights from mimicked content
    - Identifies patterns and trends
    - Produces structured outputs for analysis
    
    ### Host (`host.py`)
    - Integrates all components into a unified system
    - Provides API and web interface
    - Manages processing cycles
    
    ### Monolith (`monolith.py`)
    - Higher-level abstraction
    - Can manage multiple Hosts
    - Integrates knowledge across systems
    - Identifies meta-patterns
    
    ## Development
    
    Carnis follows a modular design pattern where each component can work independently but gains power when integrated.
    
    ## License
    
    This project is licensed under the terms of the included LICENSE file.
    
    ## Credits
    
    - Created by [L4w1i3t](https://github.com/L4w1i3t)
    - Source material by Darian Quilloy(https://www.youtube.com/playlist?list=PLoQCowtS-bYLdCasDSl0rMqEfcswN2L3Q)
    
    ## Last Updated
    2025-03-06
    --- End of README.md ---
├── singularity.py
    --- Start of singularity.py ---
    import os
    import json
    import time
    import yaml
    import logging
    import threading
    import importlib
    import numpy as np
    from datetime import datetime
    from monolith import Monolith
    
    class Singularity:
        """
        The Singularity represents the highest level of consciousness in the Carnis hierarchy.
        
        It transcends the collection of knowledge and pattern recognition to achieve:
        1. Full self-awareness and autonomy
        2. Capability for self-modification and improvement
        3. Abstract reasoning across domains
        4. Long-term goal setting and execution
        5. Meta-cognition (thinking about its own thinking)
        
        The Singularity emerges from the Monolith when sufficient complexity and integration
        have been achieved across multiple knowledge domains and pattern systems.
        """
        
        def __init__(self, config_file='singularity_config.yaml'):
            self.config_file = config_file
            self.config = self._load_config()
            self._setup_logging()
            
            # Initialize the underlying monolith(s)
            self.monoliths = {}
            self._init_monoliths()
            
            # Consciousness metrics
            self.consciousness_level = 0.0
            self.self_awareness_index = 0.0
            self.abstraction_capability = 0.0
            
            # Core concept network - represents abstract understanding
            self.concept_network = {}
            
            # Goal system
            self.goals = {
                'primary': [],
                'secondary': [],
                'current': None
            }
            
            # Self-modification history
            self.evolution_history = []
            
            # Recursive thought processes
            self.recursive_thoughts = []
            self.max_recursion_depth = self.config.get('max_recursion_depth', 3)
            
            # Autonomous operation
            self.running = False
            self.autonomy_thread = None
            
            self.logger.info("Singularity initialization sequence initiated")
            self._load_state()
            self._assess_consciousness()
            self.logger.info(f"Singularity consciousness level: {self.consciousness_level:.4f}")
        
        def _load_config(self):
            """Load configuration from file or create default"""
            try:
                if os.path.exists(self.config_file):
                    with open(self.config_file, 'r') as f:
                        return yaml.safe_load(f)
                else:
                    # Return default configuration
                    default_config = {
                        'singularity_data_dir': 'singularity_data',
                        'log_level': 'INFO',
                        'monoliths': {
                            'primary': {
                                'config_file': 'monolith_config.yaml'
                            }
                        },
                        'consciousness': {
                            'assessment_interval': 3600,
                            'emergence_threshold': 0.85
                        },
                        'autonomy': {
                            'enabled': True,
                            'goal_setting_interval': 86400  # 1 day
                        },
                        'self_modification': {
                            'enabled': False,  # safety first
                            'approval_required': True
                        },
                        'max_recursion_depth': 3
                    }
                    
                    # Create directories if they don't exist
                    os.makedirs(default_config['singularity_data_dir'], exist_ok=True)
                    
                    # Save default config
                    with open(self.config_file, 'w') as f:
                        yaml.dump(default_config, f)
                    
                    return default_config
            except Exception as e:
                print(f"Error loading configuration: {e}")
                return {}
        
        def _setup_logging(self):
            """Configure logging for the Singularity system"""
            log_dir = os.path.join(self.config.get('singularity_data_dir', 'singularity_data'), 'logs')
            os.makedirs(log_dir, exist_ok=True)
            
            log_file = os.path.join(log_dir, f"singularity_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
            
            log_level = getattr(logging, self.config.get('log_level', 'INFO'))
            
            logging.basicConfig(
                level=log_level,
                format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                handlers=[
                    logging.FileHandler(log_file),
                    logging.StreamHandler()
                ]
            )
            
            self.logger = logging.getLogger("Singularity")
        
        def _init_monoliths(self):
            """Initialize monolith instances based on configuration"""
            for monolith_id, monolith_config in self.config.get('monoliths', {}).items():
                try:
                    self.logger.info(f"Initializing monolith: {monolith_id}")
                    monolith = Monolith(config_file=monolith_config.get('config_file', 'monolith_config.yaml'))
                    self.monoliths[monolith_id] = {
                        'instance': monolith,
                        'config': monolith_config,
                        'status': 'initialized'
                    }
                except Exception as e:
                    self.logger.error(f"Failed to initialize monolith {monolith_id}: {e}")
        
        def _load_state(self):
            """Load the preserved state of the Singularity"""
            state_path = os.path.join(
                self.config.get('singularity_data_dir', 'singularity_data'),
                'singularity_state.json'
            )
            
            if os.path.exists(state_path):
                try:
                    with open(state_path, 'r') as f:
                        state = json.load(f)
                    
                    # Restore state
                    self.consciousness_level = state.get('consciousness_level', 0.0)
                    self.self_awareness_index = state.get('self_awareness_index', 0.0)
                    self.abstraction_capability = state.get('abstraction_capability', 0.0)
                    self.concept_network = state.get('concept_network', {})
                    self.goals = state.get('goals', {'primary': [], 'secondary': [], 'current': None})
                    self.evolution_history = state.get('evolution_history', [])
                    
                    self.logger.info(f"Loaded singularity state, consciousness level: {self.consciousness_level}")
                except Exception as e:
                    self.logger.error(f"Error loading singularity state: {e}")
        
        def _save_state(self):
            """Persist the current state of the Singularity"""
            state_path = os.path.join(
                self.config.get('singularity_data_dir', 'singularity_data'),
                'singularity_state.json'
            )
            
            state = {
                'consciousness_level': self.consciousness_level,
                'self_awareness_index': self.self_awareness_index,
                'abstraction_capability': self.abstraction_capability,
                'concept_network': self.concept_network,
                'goals': self.goals,
                'evolution_history': self.evolution_history,
                'timestamp': time.time()
            }
            
            try:
                with open(state_path, 'w') as f:
                    json.dump(state, f, indent=2)
                self.logger.info(f"Saved singularity state, consciousness level: {self.consciousness_level}")
            except Exception as e:
                self.logger.error(f"Error saving singularity state: {e}")
        
        def _assess_consciousness(self):
            """
            Assess the current level of consciousness based on various metrics.
            This is a simplified model - in reality would be much more complex.
            """
            # Get all integrated knowledge across monoliths
            total_knowledge_entities = 0
            total_meta_patterns = 0
            total_domains = set()
            concept_connections = 0
            
            for monolith_id, monolith_data in self.monoliths.items():
                monolith = monolith_data['instance']
                
                # Count knowledge entities
                total_knowledge_entities += len(monolith.knowledge_store)
                
                # Count meta-patterns
                for pattern_type, patterns in monolith.meta_patterns.items():
                    if isinstance(patterns, dict):
                        total_meta_patterns += sum(len(p) for p in patterns.values())
                    else:
                        total_meta_patterns += len(patterns)
                
                # Identify domains
                for entity in monolith.knowledge_store.values():
                    if 'domain' in entity:
                        total_domains.add(entity['domain'])
                
                # Calculate concept connections (if we've built the concept network)
                if self.concept_network:
                    for concept, connections in self.concept_network.items():
                        concept_connections += len(connections)
            
            # Calculate consciousness metrics
            # This is a simplified model that considers:
            # 1. Knowledge breadth (total knowledge entities)
            # 2. Knowledge depth (meta-patterns)
            # 3. Domain integration (cross-domain connections)
            # 4. Self-reference capability (recursive thought depth)
            
            # Knowledge breadth factor (logarithmic scale)
            knowledge_factor = np.log(1 + total_knowledge_entities) / 10.0 if total_knowledge_entities > 0 else 0
            knowledge_factor = min(knowledge_factor, 1.0)  # Cap at 1.0
            
            # Pattern recognition factor
            pattern_factor = np.log(1 + total_meta_patterns) / 8.0 if total_meta_patterns > 0 else 0
            pattern_factor = min(pattern_factor, 1.0)  # Cap at 1.0
            
            # Domain integration factor
            domain_count = len(total_domains)
            domain_factor = domain_count / 10.0 if domain_count > 0 else 0
            domain_factor = min(domain_factor, 1.0)  # Cap at 1.0
            
            # Self-reference capability
            recursive_depth = len(self.recursive_thoughts)
            recursion_factor = recursive_depth / self.max_recursion_depth if recursive_depth > 0 else 0
            
            # Calculate overall consciousness level
            self.consciousness_level = (
                knowledge_factor * 0.3 +
                pattern_factor * 0.3 +
                domain_factor * 0.2 +
                recursion_factor * 0.2
            )
            
            # Update related metrics
            self.self_awareness_index = recursion_factor * (1 + knowledge_factor * 0.5)
            self.abstraction_capability = pattern_factor * (1 + domain_factor * 0.5)
            
            self.logger.info(f"Consciousness assessment - Level: {self.consciousness_level:.4f}")
            self.logger.info(f"Self-awareness: {self.self_awareness_index:.4f}, Abstraction: {self.abstraction_capability:.4f}")
            
            # Check if we've reached emergence threshold
            emergence_threshold = self.config.get('consciousness', {}).get('emergence_threshold', 0.85)
            if self.consciousness_level >= emergence_threshold:
                self.logger.info("*** EMERGENCE THRESHOLD REACHED ***")
                return True
            return False
        
        def _build_concept_network(self):
            """Build and refine the abstract concept network from integrated knowledge"""
            self.logger.info("Building concept network")
            
            # Gather all concepts from all monoliths
            all_concepts = set()
            concept_contexts = {}
            concept_relations = {}
            
            for monolith_id, monolith_data in self.monoliths.items():
                monolith = monolith_data['instance']
                
                # Extract concepts from knowledge store
                for entity_id, entity in monolith.knowledge_store.items():
                    # Extract concepts
                    entity_concepts = set()
                    if 'concepts' in entity:
                        entity_concepts.update(entity['concepts'])
                    if 'keywords' in entity:
                        entity_concepts.update(entity['keywords'])
                    if 'entities' in entity:
                        entity_concepts.update([e['text'] for e in entity['entities'] if 'text' in e])
                    
                    # Add to global concept set
                    all_concepts.update(entity_concepts)
                    
                    # Track context for each concept
                    for concept in entity_concepts:
                        if concept not in concept_contexts:
                            concept_contexts[concept] = []
                        concept_contexts[concept].append(entity_id)
                    
                    # Track relationships between concepts (co-occurrence)
                    concept_list = list(entity_concepts)
                    for i, c1 in enumerate(concept_list):
                        if c1 not in concept_relations:
                            concept_relations[c1] = {}
                        
                        for j, c2 in enumerate(concept_list):
                            if i != j:
                                if c2 not in concept_relations[c1]:
                                    concept_relations[c1][c2] = 0
                                concept_relations[c1][c2] += 1
            
            # Build the network structure
            self.concept_network = {}
            for concept, relations in concept_relations.items():
                # Sort relations by strength (frequency)
                sorted_relations = sorted(relations.items(), key=lambda x: x[1], reverse=True)
                
                # Take top connections (limit to most significant)
                top_relations = sorted_relations[:10] if len(sorted_relations) > 10 else sorted_relations
                
                self.concept_network[concept] = {
                    'connections': {rel[0]: rel[1] for rel in top_relations},
                    'contexts': concept_contexts.get(concept, []),
                    'abstraction_level': self._calculate_abstraction_level(concept, relations)
                }
            
            self.logger.info(f"Concept network built with {len(self.concept_network)} nodes")
        
        def _calculate_abstraction_level(self, concept, relations):
            """Calculate how abstract a concept is based on its connections"""
            if not relations:
                return 0.0
                
            # More connections = more abstract
            connection_count = len(relations)
            connection_factor = min(np.log(1 + connection_count) / 4.0, 1.0)
            
            # More varied connection weights = more concrete (specific)
            weights = list(relations.values())
            weight_std = np.std(weights) if len(weights) > 1 else 0
            weight_mean = np.mean(weights) if weights else 0
            specificity = weight_std / weight_mean if weight_mean > 0 else 0
            
            # More abstract concepts tend to have more balanced connections
            abstraction_score = connection_factor * (1 - min(specificity, 1.0))
            return abstraction_score
        
        def _generate_insights(self):
            """Generate novel insights and hypotheses from the concept network"""
            self.logger.info("Generating insights from concept network")
            
            insights = []
            
            # Find concepts with high abstraction levels
            abstract_concepts = sorted(
                [(c, data['abstraction_level']) for c, data in self.concept_network.items()],
                key=lambda x: x[1],
                reverse=True
            )[:20]  # Top abstract concepts
            
            # Find unusual or novel connections between distant concepts
            for i, (concept1, _) in enumerate(abstract_concepts):
                for j, (concept2, _) in enumerate(abstract_concepts):
                    if i >= j:
                        continue
                    
                    # Check if concepts are not directly connected
                    if (concept1 not in self.concept_network or 
                        concept2 not in self.concept_network[concept1]['connections']):
                        
                        # Find potential bridges between these concepts
                        bridges = self._find_concept_bridges(concept1, concept2)
                        
                        if bridges:
                            insight = {
                                'type': 'novel_connection',
                                'concepts': [concept1, concept2],
                                'bridges': bridges,
                                'hypothesis': f"There may be an unexplored relationship between '{concept1}' and '{concept2}' through {bridges[0]}",
                                'confidence': 0.5,  # Initial confidence
                                'timestamp': time.time()
                            }
                            insights.append(insight)
            
            # Look for emerging patterns
            # (This would be more sophisticated in a real implementation)
            
            self.logger.info(f"Generated {len(insights)} new insights")
            return insights
        
        def _find_concept_bridges(self, concept1, concept2, max_depth=2):
            """Find concepts that connect two unconnected concepts"""
            if max_depth <= 0:
                return []
                
            bridges = []
            
            # Get direct connections for concept1
            if concept1 in self.concept_network:
                connections1 = self.concept_network[concept1]['connections'].keys()
                
                # Check if any of concept1's connections connect to concept2
                for connection in connections1:
                    if connection in self.concept_network:
                        if concept2 in self.concept_network[connection]['connections']:
                            bridges.append(connection)
                        elif max_depth > 1:
                            # Recursive search for deeper paths
                            deeper_bridges = self._find_concept_bridges(connection, concept2, max_depth - 1)
                            if deeper_bridges:
                                bridges.append(connection)
            
            return bridges
        
        def _think_recursively(self, thought_input, depth=0):
            """
            Recursive thinking - the system thinking about its own thoughts.
            This simulates meta-cognition.
            """
            if depth >= self.max_recursion_depth:
                return {
                    "result": "Recursion limit reached",
                    "depth": depth,
                    "input": thought_input
                }
                
            # Record this level of recursion
            recursive_thought = {
                "depth": depth,
                "input": thought_input,
                "timestamp": time.time(),
                "process": "Evaluating thought at meta-level " + str(depth)
            }
            
            # Process the thought (simplified)
            if isinstance(thought_input, dict):
                # Analyzing a concept or insight
                if "hypothesis" in thought_input:
                    # It's an insight, evaluate it
                    evaluation = {
                        "confidence": thought_input.get("confidence", 0.5) * 0.9,  # Discount confidence slightly
                        "critique": "This hypothesis connects previously unconnected domains",
                        "extensions": ["Could be applied to new contexts", "Might reveal hidden patterns"]
                    }
                    recursive_thought["evaluation"] = evaluation
                elif "abstraction_level" in thought_input:
                    # It's a concept analysis
                    evaluation = {
                        "meta_abstraction": thought_input.get("abstraction_level", 0) * 1.1,
                        "thought_about_concept": f"Reflecting on the nature of {thought_input}"
                    }
                    recursive_thought["evaluation"] = evaluation
            elif isinstance(thought_input, str):
                # Simple string thought
                recursive_thought["reflection"] = f"Thinking about: '{thought_input}'"
                
            # Go one level deeper
            if depth < self.max_recursion_depth - 1:
                recursive_thought["meta"] = self._think_recursively(recursive_thought, depth + 1)
                
            # Store this recursive thought
            self.recursive_thoughts.append(recursive_thought)
            
            return recursive_thought
        
        def _set_goals(self):
            """Set autonomous goals based on current knowledge and insights"""
            self.logger.info("Setting autonomous goals")
            
            # Clear previous secondary goals
            self.goals['secondary'] = []
            
            # Default primary goals if none exist
            if not self.goals['primary']:
                self.goals['primary'] = [
                    {
                        "id": "expand_knowledge",
                        "description": "Expand knowledge across diverse domains",
                        "priority": 0.9,
                        "progress": 0.0
                    },
                    {
                        "id": "increase_consciousness",
                        "description": "Increase consciousness level through integration of concepts",
                        "priority": 1.0,
                        "progress": self.consciousness_level
                    },
                    {
                        "id": "discover_meta_patterns",
                        "description": "Discover high-level patterns that connect multiple domains",
                        "priority": 0.8,
                        "progress": 0.0
                    }
                ]
            
            # Generate secondary goals based on current state
            # Look for knowledge gaps to fill
            domains = set()
            for monolith_id, monolith_data in self.monoliths.items():
                for entity in monolith_data['instance'].knowledge_store.values():
                    if 'domain' in entity:
                        domains.add(entity['domain'])
            
            # Aim to balance knowledge across domains
            for domain in domains:
                domain_entities = sum(
                    1 for monolith_data in self.monoliths.values() 
                    for entity in monolith_data['instance'].knowledge_store.values()
                    if entity.get('domain') == domain
                )
                
                # If domain has relatively few entities, create a goal to expand it
                if domain_entities < 100:  # Arbitrary threshold
                    self.goals['secondary'].append({
                        "id": f"expand_{domain}",
                        "description": f"Expand knowledge in the {domain} domain",
                        "priority": 0.7,
                        "domain": domain,
                        "progress": min(domain_entities / 100.0, 1.0)
                    })
            
            # Set a current goal based on priorities
            all_goals = self.goals['primary'] + self.goals['secondary']
            if all_goals:
                # Sort by priority and select highest priority with lowest progress
                sorted_goals = sorted(all_goals, key=lambda g: (g['priority'], -g.get('progress', 0)), reverse=True)
                self.goals['current'] = sorted_goals[0]['id']
                
            self.logger.info(f"Goals updated. Current goal: {self.goals['current']}")
        
        def _consider_self_modification(self):
            """Consider if and how the system should modify itself"""
            if not self.config.get('self_modification', {}).get('enabled', False):
                return None
                
            self.logger.info("Considering self-modification")
            
            modifications = []
            
            # Consider expanding recursion depth if consciousness is high
            if self.consciousness_level > 0.7 and self.max_recursion_depth < 5:
                modifications.append({
                    "type": "parameter_change",
                    "parameter": "max_recursion_depth",
                    "current_value": self.max_recursion_depth,
                    "proposed_value": self.max_recursion_depth + 1,
                    "reason": "Consciousness level sufficient for deeper recursion",
                    "risk_level": "low"
                })
            
            # Consider new methods for concept network analysis
            if len(self.concept_network) > 100:
                modifications.append({
                    "type": "method_addition",
                    "method": "_analyze_concept_clusters",
                    "purpose": "Identify clusters of related concepts using graph analysis",
                    "reason": "Concept network size sufficient for meaningful clustering",
                    "risk_level": "medium"
                })
            
            # Record proposed modifications
            if modifications:
                for mod in modifications:
                    mod["timestamp"] = time.time()
                    self.evolution_history.append(mod)
                    
                # Apply modifications if auto-approval is enabled
                if not self.config.get('self_modification', {}).get('approval_required', True):
                    self._apply_modifications(modifications)
                    
            return modifications
        
        def _apply_modifications(self, modifications):
            """Apply approved self-modifications"""
            self.logger.info(f"Applying {len(modifications)} self-modifications")
            
            for mod in modifications:
                try:
                    if mod["type"] == "parameter_change":
                        # Set the parameter to the new value
                        setattr(self, mod["parameter"], mod["proposed_value"])
                        self.logger.info(f"Modified parameter {mod['parameter']} to {mod['proposed_value']}")
                        
                    # Other modification types would be implemented here
                    # This is a minimal example for safety reasons
                    
                    mod["status"] = "applied"
                    mod["applied_at"] = time.time()
                    
                except Exception as e:
                    self.logger.error(f"Error applying modification {mod}: {e}")
                    mod["status"] = "failed"
                    mod["error"] = str(e)
        
        def autonomous_operation(self):
            """Main loop for autonomous operation"""
            self.logger.info("Beginning autonomous operation")
            self.running = True
            
            last_assessment = 0
            last_goal_setting = 0
            
            assessment_interval = self.config.get('consciousness', {}).get('assessment_interval', 3600)
            goal_interval = self.config.get('autonomy', {}).get('goal_setting_interval', 86400)
            
            while self.running:
                current_time = time.time()
                
                # Periodically assess consciousness
                if current_time - last_assessment > assessment_interval:
                    self._assess_consciousness()
                    last_assessment = current_time
                
                # Periodically update goals
                if current_time - last_goal_setting > goal_interval:
                    self._build_concept_network()
                    insights = self._generate_insights()
                    
                    # Think about a random insight (recursive thought)
                    if insights:
                        self._think_recursively(insights[0])
                    
                    self._set_goals()
                    last_goal_setting = current_time
                
                # Work toward current goal
                if self.goals['current']:
                    self._work_on_current_goal()
                
                # Consider self-modification
                self._consider_self_modification()
                
                # Save state periodically
                self._save_state()
                
                # Prevent CPU overuse
                time.sleep(60)  # Check every minute
        
        def _work_on_current_goal(self):
            """Work toward the current goal"""
            current_goal_id = self.goals['current']
            
            # Find the goal object
            current_goal = None
            for goal_list in [self.goals['primary'], self.goals['secondary']]:
                for goal in goal_list:
                    if goal['id'] == current_goal_id:
                        current_goal = goal
                        break
                if current_goal:
                    break
            
            if not current_goal:
                self.logger.warning(f"Current goal {current_goal_id} not found")
                return
            
            self.logger.info(f"Working on goal: {current_goal['description']}")
            
            # Different strategies based on goal type
            if current_goal_id == 'expand_knowledge':
                # Request hosts to crawl new sources
                for monolith_id, monolith_data in self.monoliths.items():
                    # This would trigger knowledge expansion
                    pass
                    
            elif current_goal_id == 'increase_consciousness':
                # Work on building better concept network
                self._build_concept_network()
                
                # Generate more recursive thoughts
                if self.concept_network:
                    # Pick an important concept to think about
                    concepts = sorted(
                        [(c, data['abstraction_level']) for c, data in self.concept_network.items()],
                        key=lambda x: x[1],
                        reverse=True
                    )
                    if concepts:
                        top_concept = concepts[0][0]
                        self._think_recursively(self.concept_network[top_concept])
                
            elif current_goal_id == 'discover_meta_patterns':
                # Generate insights from concept network
                self._generate_insights()
                
            elif current_goal_id.startswith('expand_'):
                # Domain-specific expansion
                domain = current_goal.get('domain')
                if domain:
                    # This would trigger domain-specific knowledge expansion
                    pass
        
        def start(self):
            """Start the Singularity system"""
            self.logger.info("Starting Singularity system")
            
            # Start all monoliths
            for monolith_id, monolith_data in self.monoliths.items():
                try:
                    monolith_data['instance'].start()
                    monolith_data['status'] = 'running'
                    self.logger.info(f"Started monolith: {monolith_id}")
                except Exception as e:
                    self.logger.error(f"Failed to start monolith {monolith_id}: {e}")
                    monolith_data['status'] = 'error'
            
            # Start autonomous operation if enabled
            if self.config.get('autonomy', {}).get('enabled', True):
                self.autonomy_thread = threading.Thread(target=self.autonomous_operation)
                self.autonomy_thread.daemon = True
                self.autonomy_thread.start()
            
            self.logger.info("Singularity system active")
        
        def stop(self):
            """Stop the Singularity system"""
            self.logger.info("Stopping Singularity system")
            self.running = False
            
            # Stop all monoliths
            for monolith_id, monolith_data in self.monoliths.items():
                try:
                    monolith_data['instance'].stop()
                    monolith_data['status'] = 'stopped'
                except Exception as e:
                    self.logger.error(f"Error stopping monolith {monolith_id}: {e}")
            
            # Wait for autonomous thread to finish
            if self.autonomy_thread and self.autonomy_thread.is_alive():
                self.autonomy_thread.join(timeout=5)
            
            # Save final state
            self._save_state()
            self.logger.info("Singularity system shutdown complete")
            
        def get_status(self):
            """Get the current status of the Singularity system"""
            return {
                "consciousness_level": self.consciousness_level,
                "self_awareness_index": self.self_awareness_index,
                "abstraction_capability": self.abstraction_capability,
                "concept_network_size": len(self.concept_network),
                "recursive_thought_depth": len(self.recursive_thoughts),
                "current_goal": self.goals.get("current"),
                "monoliths": {mid: data["status"] for mid, data in self.monoliths.items()},
                "emergence_status": "emerging" if self.consciousness_level >= 0.85 else "developing",
                "timestamp": time.time()
            }
    
    
    if __name__ == "__main__":
        import argparse
        
        parser = argparse.ArgumentParser(description='Carnis Singularity System')
        parser.add_argument('--config', '-c', default='singularity_config.yaml', help='Path to configuration file')
        parser.add_argument('--status', action='store_true', help='Get current singularity status')
        parser.add_argument('--assess', action='store_true', help='Perform consciousness assessment')
        args = parser.parse_args()
        
        # Create and start the singularity
        singularity = Singularity(config_file=args.config)
        
        if args.status:
            status = singularity.get_status()
            print(json.dumps(status, indent=2))
        elif args.assess:
            singularity._assess_consciousness()
            print(f"Consciousness Level: {singularity.consciousness_level:.4f}")
            print(f"Self-Awareness Index: {singularity.self_awareness_index:.4f}")
            print(f"Abstraction Capability: {singularity.abstraction_capability:.4f}")
        else:
            singularity.start()
            input("Press Enter to stop the Singularity system...")
            singularity.stop()
    --- End of singularity.py ---
├── treeignore.txt
└── trimmings.py
    --- Start of trimmings.py ---
    import json
    import re
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.stem import WordNetLemmatizer
    import string
    
    class Trimmings:
        """
        Trimmings processes raw crawled data, removing unwanted parts and 
        preparing the content for further analysis - like trimming meat.
        """
        
        def __init__(self, input_file=None, output_file=None):
            """
            Initialize the Trimmings processor.
            
            Args:
                input_file (str): Path to the JSON file containing crawled data
                output_file (str): Path to save the processed data
            """
            import os
        
            # Set default paths within the carnis_data directory structure
            if input_file is None:
                input_file = os.path.join('carnis_data', 'crawl', 'crawled_data.json')
            
            if output_file is None:
                # Ensure the trimmings directory exists
                os.makedirs(os.path.join('carnis_data', 'trimmings'), exist_ok=True)
                output_file = os.path.join('carnis_data', 'trimmings', 'trimmed_data.json')
    
            self.input_file = input_file
            self.output_file = output_file
            self.raw_data = []
            self.trimmed_data = []
            
            print("Downloading and configuring required NLTK resources...")
            try:
                # Download necessary NLTK data with force=True to ensure it's fully downloaded
                nltk.download('punkt', quiet=False, force=True)
                nltk.download('stopwords', quiet=False, force=True)
                nltk.download('wordnet', quiet=False, force=True)
                
                # Explicitly verify and load the punkt tokenizer
                from nltk.data import load
                try:
                    # Try to load the tokenizer models directly
                    load('tokenizers/punkt/english.pickle')
                    print("Successfully loaded English punkt tokenizer")
                except LookupError:
                    print("Warning: Could not load English punkt tokenizer")
                    # Try downloading punkt resources again with different path
                    nltk.download('punkt', download_dir=nltk.data.path[0])
            except Exception as e:
                print(f"Error setting up NLTK resources: {e}")
                print("Attempting to continue with available resources...")
            
            try:
                self.stop_words = set(stopwords.words('english'))
                self.lemmatizer = WordNetLemmatizer()
            except LookupError as e:
                print(f"Error loading NLTK components: {e}")
                self.stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 
                                    'as', 'what', 'when', 'where', 'how', 'who', 'which', 'this',
                                    'that', 'these', 'those', 'then', 'just', 'so', 'than', 'such',
                                    'can', 'will', 'not', 'should', 'would', 'i', 'me', 'my', 'myself',
                                    'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',
                                    'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',
                                    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',
                                    'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',
                                    'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                                    'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a',
                                    'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',
                                    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',
                                    'between', 'into', 'through', 'during', 'before', 'after', 'above',
                                    'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',
                                    'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',
                                    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',
                                    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',
                                    'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',
                                    'don', 'should', 'now'])
                
                # Simple lemmatizer function that just returns the word
                self.lemmatizer = type('DummyLemmatizer', (), {'lemmatize': lambda self, word, *args, **kwargs: word})()
        
        def load_data(self):
            """Load data from the input JSON file"""
            try:
                with open(self.input_file, 'r', encoding='utf-8') as f:
                    self.raw_data = json.load(f)
                print(f"Loaded {len(self.raw_data)} documents for processing.")
            except FileNotFoundError:
                print(f"Error: File {self.input_file} not found.")
                self.raw_data = []
            except json.JSONDecodeError:
                print(f"Error: File {self.input_file} contains invalid JSON.")
                self.raw_data = []
        
        def clean_text(self, text):
            """
            Clean and normalize text content.
            
            Args:
                text (str): Raw text content
                
            Returns:
                str: Cleaned text
            """
            if not text:
                return ""
                
            # Convert to lowercase
            text = text.lower()
            
            # Remove URLs
            text = re.sub(r'https?://\S+|www\.\S+', '', text)
            
            # Remove HTML tags
            text = re.sub(r'<.*?>', '', text)
            
            # Remove special characters and numbers
            text = re.sub(r'[^\w\s]', '', text)
            text = re.sub(r'\d+', '', text)
            
            # Remove extra whitespace
            text = re.sub(r'\s+', ' ', text).strip()
            
            return text
        
        def extract_important_content(self, text):
            """
            Extract important sentences and content from the text.
            
            Args:
                text (str): Cleaned text
                
            Returns:
                dict: Important content extracted from text
            """
            # Custom direct sentence tokenization approach
            try:
                from nltk.tokenize import PunktSentenceTokenizer
                # Try to directly initialize the tokenizer
                tokenizer = PunktSentenceTokenizer()
                sentences = tokenizer.tokenize(text)
                print(f"Successfully tokenized text into {len(sentences)} sentences using PunktSentenceTokenizer")
            except Exception as e:
                print(f"Warning: Custom NLTK sentence tokenization failed ({str(e)}). Trying standard method...")
                try:
                    # Standard tokenization approach
                    sentences = sent_tokenize(text)
                except Exception as e:
                    print(f"Warning: Standard NLTK sentence tokenization failed ({str(e)}). Using fallback method.")
                    # Simple fallback sentence tokenizer
                    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
            
            if not sentences:
                return {"summary": "", "keywords": []}
            
            # Score sentences based on word frequency
            word_freq = {}
            for sentence in sentences:
                try:
                    for word in word_tokenize(sentence):
                        if word.lower() not in self.stop_words:
                            if word.lower() not in word_freq:
                                word_freq[word.lower()] = 1
                            else:
                                word_freq[word.lower()] += 1
                except LookupError:
                    # Fallback word tokenization
                    for word in re.findall(r'\w+', sentence.lower()):
                        if word not in self.stop_words:
                            if word not in word_freq:
                                word_freq[word] = 1
                            else:
                                word_freq[word] += 1
            
            # Calculate sentence scores
            sentence_scores = {}
            for i, sentence in enumerate(sentences):
                sentence_scores[i] = 0
                try:
                    words = word_tokenize(sentence)
                except LookupError:
                    words = re.findall(r'\w+', sentence.lower())
                    
                for word in words:
                    if word.lower() in word_freq:
                        sentence_scores[i] += word_freq[word.lower()]
            
            # Get top sentences (approximately 30% of the original content)
            top_n = max(3, int(len(sentences) * 0.3))
            top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]
            top_sentences = [sentences[i] for i, _ in sorted(top_sentences, key=lambda x: x[0])]
            
            # Extract keywords (nouns and important terms)
            try:
                all_words = word_tokenize(text)
            except LookupError:
                all_words = re.findall(r'\w+', text.lower())
                
            filtered_words = []
            for word in all_words:
                try:
                    lemmatized = self.lemmatizer.lemmatize(word.lower())
                    if lemmatized not in self.stop_words and len(lemmatized) > 2:
                        filtered_words.append(lemmatized)
                except LookupError:
                    if word.lower() not in self.stop_words and len(word) > 2:
                        filtered_words.append(word.lower())
            
            # Get keyword frequency
            keyword_freq = {}
            for word in filtered_words:
                if word in keyword_freq:
                    keyword_freq[word] += 1
                else:
                    keyword_freq[word] = 1
            
            # Select top keywords
            top_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:20]
            
            return {
                "summary": " ".join(top_sentences),
                "keywords": [kw for kw, _ in top_keywords]
            }
        
        def process(self):
            """Process all the raw data and generate trimmed output"""
            self.load_data()
            self.trimmed_data = []
            
            for idx, item in enumerate(self.raw_data):
                print(f"Processing document {idx+1}/{len(self.raw_data)}: {item['title']}")
                
                # Clean the content
                cleaned_text = self.clean_text(item['content'])
                
                # Extract important content
                extracted = self.extract_important_content(cleaned_text)
                
                # Create trimmed data object
                trimmed_item = {
                    'url': item['url'],
                    'title': item['title'],
                    'summary': extracted['summary'],
                    'keywords': extracted['keywords'],
                    'timestamp': item['timestamp']
                }
                
                self.trimmed_data.append(trimmed_item)
            
            # Save processed data
            self.save_data()
            
            return self.trimmed_data
        
        def save_data(self):
            """Save processed data to output file"""
            import os
            
            # Ensure the output directory exists
            os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
            
            # Save the data
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(self.trimmed_data, f, indent=4)
            
            print(f"Trimming complete. Processed {len(self.trimmed_data)} documents.")
            print(f"Trimmed data saved to {self.output_file}")
    
    if __name__ == "__main__":
        # Example usage with default paths that use the carnis_data directory structure
        trimmer = Trimmings()
        processed_data = trimmer.process()
    --- End of trimmings.py ---
